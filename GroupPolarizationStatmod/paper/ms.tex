% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  abstract]{article}

\usepackage{amsmath,amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{iftex}
\input{preamble.tex}
\hypersetup{
  pdftitle={Group polarization replications may be marred by high false discovery rates},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue}}


\newcommand{\mupre}{\mu_\mathrm{pre}}
\newcommand{\mupost}{\mu_\mathrm{post}}

\newcommand{\sigmapre}{\sigma_\mathrm{pre}}
\newcommand{\sigmapost}{\sigma_\mathrm{post}}

\newcommand{\thetafit}{\theta^\mathrm{fit}}

\newcommand{\fdr}{\mathrm{FDR}}

\newcommand{\meanobs}{\bar{o}}
\newcommand{\meanobsemp}{\bar{\mathbf{o}}}
\newcommand{\meanobst}{\meanobs_t}
\newcommand{\meanobsjt}{\meanobs_{j,t}}

\newcommand{\meanobspre}{\meanobs_\mathrm{pre}}
\newcommand{\meanobspost}{\meanobs_\mathrm{post}}

\definecolor{myorange}{RGB}{240, 96, 0}
\newcommand{\mt}[1]{{\textcolor{myorange} {({\tiny MT:} #1)}}}

\usepackage{authblk}
\usepackage{etoolbox}
\usepackage{wrapfig}
\makeatletter
\title{If the Null Fits, You Must Omit: Ubiquitous False Detections of Group Polarization}
\makeatother
\author[1,*]{{Matthew A.~Turner}}
\affil[1]{\small Environmental Social Sciences, Stanford Doerr School of Sustainability, Stanford University}

\author[2,3]{{Paul E.~Smaldino}}
\affil[2]{\small Cognitive and Information Sciences, University of California, Merced}
\affil[3]{\small Santa Fe Institute} 
% \vspace{2em}
\affil[*]{\small Correspondence: \href{mailto:maturner@stanford.edu}{maturner@stanford.edu}}

\date{}
\begin{document}
\maketitle
% \begin{abstract}
% \noindent 
% \emph{Group polarization} is the name for a form of consensus where
% members of a like-minded group become, on average, more extreme in their
% opinions after discussing a topic. Group polarization is important because
% it may increase social and political tensions if moderates become more
% extreme. Decades of studies have replicated detections of group
% polarization, typically using an ordinal scale to measure opinions.
% However, these studies did not account for ordinal measurements, which can
% result in spurious detections of opinion change.  Lacking original data we
% can still calculate the probability that a published detection of group
% polarization was spurious, i.e., to calculate the \emph{false detection
% rate}.  Across 54 group polarization experimental conditions from ten
% representative journal articles, we calculated false detection rates to be
% between 0.52 and 0.88, with a median of 0.75, using a generative model of
% group polarization experiments seeded with empirical data.  
% We also use our model to develop experimental designs that
% achieve an acceptable false discovery rate.  Much of group polarization
% research may be unreliable. This analysis can help change that by enabling others to avoid this
% hidden pitfall. More
% broadly this work demonstrates one important way that replication success alone
% does not imply epistemic reliability.
% \end{abstract}

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=gray}
\setcounter{tocdepth}{2}
\tableofcontents
}

\begin{center} \noindent\rule{4cm}{0.4pt} \end{center}
% \clearpage

\begin{quote}
In our introductory social psychology course, 
we have for many years used the [group polarization experimental paradigm] as
a laboratory exercise. The exercise works beautifully, but one must be
careful to forewarn a class that [group polarization] does not occur with every 
group\ldots and that the effect is not large. 
\par\raggedleft(Brown, 1986, p.\cite[p. 205]{Brown1986})
\end{quote}

\begin{quote}
One of the most robust findings in social psychology is that of attitude polarization 
following discussion with like-minded others.
\par\raggedleft(Cooper et al, 2001, p. 267 \cite[p. 267]{Cooper2001})
\end{quote}

\begin{quote}
A medium effect size is conceived as one large enough to be visible to the naked eye.
\par\raggedleft(Cohen, 1988 \cite[p. 26]{CohenBook1988})
\end{quote}

\input{intro-latest.tex}

% \input{old-intro.tex}

\section{Methods}\label{methods}



\begin{figure}
  \centering
    % \includegraphics[width=0.8\textwidth]{Figures/Model/data-model-flow.png}
    \includegraphics[width=\textwidth]{Figures/Model/data-model-flow-w-webapp.pdf}
  \caption{\textbf{Workflow of the GP-stats simulation and analysis pipeline
  for estimating false detection rates under null-consistent conditions.} Colored boxes indicate file types: teal = R scripts, blue = data files, orange = plotting scripts, cyan
= Slurm batch script. Arrows show the flow from parameter estimation (hillclimbing or Shiny app) to Bayesian null model fitting on the cluster, aggregation of simulated results,
and final visualization. (D) shows the web app interface that takes researcher guesses at
simple consensus models that generate observations, and the app passes the guess to initialize
the hillclimbing search algorithm for finding spurious group polarization.}
  \label{fig:data-model-flow.png}
\end{figure}


Metascience frames our analysis since metascientific theory provides a framework for
our goal of calculating the false discovery rate across experimental conditions,
journal articles, and the ten-paper corpus. The false discovery rate is then
the Type I error rate relative to the rate of positive discoveries, true or false.
We calculate via simulation and data fitting
the Type I error rate for the experimental designs used in experimental conditions
that, under a preliminary large-$N$ model analysis, generated group
polarization upon ordinal measurement of simple consensus latent opinion 
distributions. These can then be aggregated to estimate the false discovery rate
for each journal article and for the ten-article corpus, which is representative of the
false discovery rate for group polarization studies. 

\subsection{Metascience of the false detection rate in group polarization}

The false detection rate is the scaled probability that a detection, $D$, of
an effect, $E$, is in fact a false detection and there is \emph{not} an effect,
denoted $\neg E$. The probability that a detection $D$ has occurred when there
was no effect, i.e. when $\neg E$, is denoted $\alpha = \Pr(D | \neg E)$.
This is known as the Type I or family-wise error rate. To calculate the 
false detection rate ($\fdr$), one must compare $\alpha$ to the statistical power,
the probability of a detection when there is an effect, denoted $W = \Pr(D | E)$.
Both these values must be scaled by the rate at which effects occur, denoted $b$.
Using the notation outlined above, the false detection rate is
\begin{equation}
  \fdr = \frac{(1 - b)\alpha}{(1 - b)\alpha + bW}.
  \label{eq:fdr}
\end{equation}


In group polarization, $E$ represents the group polarization \emph{effect}, and
$\neg E$ represents its absence. Simple consensus is one way for group polarization
to be absent, i.e., simple consensus is one form of $\neg E$. The presence of
an effect $E$ is independent of whether an effect was \emph{detected}, 
represented by $D$ if it was detected and $\neg D$ if not. 

The group polarization studies are journal articles that present
the results of multiple experiments to support some hypothesis. To organize
the presentation of false detection rates across the ten studies in our
analysis, we first will organize false detection rates by experiment, where
we created `ExperimentID` tags for each of the sixty experiments across ten
studies. We also created `StudyID` tags for each of the ten journal articles
with the key `{Author}{Year}`, e.g., \texttt{Schkade2010} for Schkade, et al.,
(2010) \cite{Schkade2010}. 

We calculate $\alpha = \Pr(D | \neg E)$ through simulation for experimental
conditions, 
and use optimistic values of
$b=0.1$~\cite{Ioannidis2005,McElreath2015} and $W=0.8$~\cite{Smaldino2016NatSel}
in the main text and in the compare with more pessimistic, yet realistic, estimates
of these values, as well as even more optimistic estimates. 

\subsection{How study design can induce false detections of group polarization}
\label{subsec:false-detections}


An opinion is never measured directly, only a behavior that indicates an opinion
in a behavioral experiment or survey.  For example, the location of a mark
that a participant makes is measured, not the complex neural activity that
underlies forming opinions (Figure~\ref{fig:experiment_model}A) \mt{Some of this
needs to be Intro'd}.
Theoretically, an internal opinion in the form of neural activity is a
\textit{latent opinion}, from the Latin \emph{latentem} meaning ``lie
hidden''.  Participants report \text{observed opinions}, typically as one of
a finite number of ranked, successive integer values that form the
\emph{selection set}. The minimum possible value in the selection set might
mean ``strongly disagree'' and the maximum value represents ``strongly
agree''.

% \begin{wrapfigure}{r}{0.5\textwidth}
\begin{figure}
  \caption{\textbf{Opinion measurement model and spurious group polarization.} 
  Latent model participant opinions are drawn from a normal distribution with the
  listed parameters (A,C). The pre-deliberation latent distribution has high
  variance, but appears polarized when measured via integration (A).  The latent
  distribution's variance is expected to decrease during deliberation (B) via
  consensus, but in this example the latent mean is static, 
$\mupre = \mupost$. However, when the post-deliberation distribution is measured,
the simulated observed mean increases, i.e., spurious
group polarization occurs (C).}
  \centering
  \includegraphics[width=0.65\textwidth]{Figures/Model/latent-ordinal-distros.pdf}
  \label{fig:distros}
\end{figure}
% \end{wrapfigure}

Group polarization experiments share a common experimental design where,
first, participant opinions are gathered on a topic of interest. Next, groups
are formed that have some initial opinion extremism. Participants deliberate
or discuss the topic for some set time. After the set time, participants
again give their opinion. Group polarization is said to occur if the
mean group opinion became more extreme, i.e., if the magnitude of the mean
group opinion is greater \emph{post-deliberation} compared to
\emph{pre-deliberation}. For example,
\cite{Moscovici1969} asked Parisian high school students to indicate their
degree of agreement with the statement, ``American economic aid is always
used for political pressure'' (Figure~\ref{fig:experiment_model}A). There
were seven options in the selection set, -3 indicating ``strongly disagree'',
+3 indicating ``strongly agree'', and 0 was neutral. Moscovici and Zavalloni
report that the mean pre-deliberation opinion was -0.61 and the mean
post-deliberation opinion was -1.09 (Moscovici and Zavalloni, 1969; Table 4,
p. 132). This is a -0.48 shift to more extreme disagreement with the 
statement, reported by Moscovici and Zavalloni as statistically significant
in Table 5 on p. 132.

We formalize this experimental design in the $N\to\infty$ case where the
distribution of ordinal measurements is created by integrating continuous latent
opinion distributions.  Latent opinions in the model are normally distributed with
mean $\mu_t$ and variance $\sigma_t$ for time $t$. We are concerned with only two
times, \emph{pre-} and \emph{post-} deliberation ($t=0$ and $t=T$, respectively).
For short we can write $\mu_\mathrm{pre}$ and $\sigma_\mathrm{pre}$ for
pre-deliberation mean and variance (and similar for \emph{post}-deliberation).  
The latent opinion of participant $i$ at time $t$ is drawn from a normal 
distribution in the model, 
\begin{equation} 
  o_{i,t} \sim p(o; \mu_t, \sigma_t) = \mathcal{N}(\mu_t, \sigma_t).
  \label{eq:opinionDistribution} 
\end{equation} 
\noindent 
Latent opinions are assumed to be normally distributed since they are unbounded 
aggregations of neurobiological activity. 

The measurement process with $N\to\infty$ participants is simulated by 
integrating the latent opinion distribution between ordinal bin thresholds, 
of which there are $B+1$, where $B$ is the number of opinion bins, with sequential
integer values $b$. In the Moscovici and Zavalloni (1969) example, $b \in
\{-3,-2,\ldots,2,3\}$ and $B = 7$.
Bin $b$ is defined by its lower and upper continuous-valued thresholds, 
$\theta_{b-1}$ and
$\theta_{b}$, respectively. Thresholds are given by the following equations
(and visualized as dotted lines in Figure~\ref{fig:distros}).
\begin{equation}
\theta_b = \begin{cases}
  -\infty         & \text{ if } b=0 \\
  \infty          & \text{ if } b=B \\
  b + \frac{1}{2} & \text{ otherwise.}
\end{cases}
\end{equation}
\noindent
With thresholds defined, the frequency of ordinal measurment $b$ at time $t$ 
is (Figure~\ref{fig:distros})
\begin{equation}
  p(b;~\mu_t, \sigma_t, B) = \int_{\theta_{b-1}}^{\theta_b} p(o; \mu_t, \sigma_t)
  do.
  \label{eq:ordinal-frequency}
\end{equation}

We can now define spurious group polarization and give examples of when it occurs.
First, the observed group polarization opinion shift is
\begin{equation}
  s = \meanobs_\mathrm{post} - \meanobs_\mathrm{pre}~,
  \label{eq:group-polarization}
\end{equation}
\noindent
where the expected value of observed opinions at time $t$ is
\begin{equation}
  \meanobst = \frac{1}{B} \sum_{b=1}^B b \cdot p(b; \mu_t, \sigma_t, B).
  \label{eq:meanobs}
\end{equation}
\noindent
\emph{Spurious} group polarization is when mean observed opinions seem to increase in 
extremity when the latent mean held constant. We simulate spurious
group polarization by setting $\mupre = \mupost = \mu$ and setting the variances
to values known to induce spurious group polarization. 
We explain how to find a constant latent mean, $\mu$, and pre- and post-deliberation
variances, $\sigmapre$ and $\sigmapost$, respectively, that generate spurious 
detections of group polarization in the next section. 
% We describe how we find $\mu$,
% $\sigmapre$ and $\sigmapost$ next in Section~\ref{sec:finding-variances}.


\subsection{Web app controls algorithm to find parameters inducing spurious group polarization} 

To calculate the false discovery rate for each experimental condition, we first must
identify a constant latent mean, $\mu$, and latent variances $\sigmapre$ and
$\sigmapost$ that generate observation distributions\mt{define earlier?} with
reported mean pre- and post-deliberation opinions and resulting shift $s$ (calculated
using Equation~\ref{eq:group-polarization}). Define the collection of these
parameters $\beta = \{\mu, \sigmapre, \sigmapost\}$. We identify $\beta$ through a
hillclimbing optimization algorithm that finds $\beta$ that minimizes the difference
between reported pre- and post-deliberation mean opinions and simulated ones
calculated with Equation~\ref{eq:meanobs}. 

The algorithm to find $\beta$ holds $\mu$ constant while searching for $\sigmapre$
and $\sigmapost$ that generate observed $\meanobspre$ and $\meanobspost$, within a
tolerance of $\pm10^{-3}$ (Algorithm~\ref{alg:hillclimbing}). 
The algorithm requires a researcher to specify an initial
guess for $\beta$ and auxiliary parameter settings, which generally vary between
experimental conditions.  Furthermore, the algorithm does not always produce
reasonable distributions with a given $\beta$, and sometimes it does not produce
spurious group polarization.  Therefore, it is necessary to sanity check the latent
and ordinal opinion distributions for the $\beta$ found by the algorithm. If all is
well, the experimental condition can be marked as plausibly spurious.  To tame the
complexity involved in data entry and model fitting for each of these sixty
experimental conditions, we developed a web application to input data from control
the algorithm and inspect its results (Figure~\ref{fig:webapp}). Web application code is
available online in our GitHub
repository\footnote{\url{https://github.com/mt-digital/gp-statmod/blob/main/GroupPolarizationStatmod/app.R}}.

\begin{algorithm}
  \caption{Routine to find latent mean and variance that generate spurious group polarization.}
  \label{alg:hillclimbing}
  \begin{algorithmic}

    \Require $\mu$, $\sigma$, $\bar{\mathbf{o}}$ 
      \Comment{Set latent mean, guess variance that yields, input empirical mean.}

    \Require $\epsilon$, $\delta$, $i_\mathrm{max}$ 
      \Comment{Hillclimbing tolerance, step size, and maximum iterations.}

    \State $\Delta \gets \meanobs(\mu, \sigma) - \meanobsemp$ 
      \Comment{Initialize simulation error.}
    \While {$\Delta^2 > \epsilon$ and $i < i_\mathrm{max}$} \Comment{Search until desired
    accuracy or max iterations reached.}
      \State $\sigma' \sim \mathcal{N}(\sigma, \delta)$ \Comment{Draw new variance to test.}
      \State $\Delta' \gets \bar{o}(\mu, \sigma') - \bar{\mathbf{o}}$ 
        \Comment{Calculate error using new variance.}
      \If {$\Delta' - \Delta < 0$}
        \State $\sigma \gets \sigma'$
          \Comment{Update $\sigma$ if the error is less than before.}
      \EndIf
      \State $\Delta \gets \bar{o}(\mu, \sigma) - \bar{\mathbf{o}}$
        \Comment{Update simulation error if $\sigma$ is updated.}
    \EndWhile \\

    \Return $\sigma$
  \end{algorithmic}
\end{algorithm}


% \begin{figure}
%   \caption{\textbf{Shiny web application for user-guided hillclimbing to find
%   parameters that generate spurious group polarization.} This web application enables users to
% create new analyses of experimental conditions from published studies. Users 
% provide initial guesses for the algorithm to use as starting points in 
% finding variances that generate spurious group polarization. Users inspect 
% model fits provided by the algorithm, adjusting parameters if necessary, and
% mark an experimental condition as ``Plausibly spurious'' if a reasonable fit
% is indeed found.\mt{Maybe add some A, B, C guides to the figure and include
% in parentheses here to go with the text?}}
%   \centering
%     \includegraphics[width=0.75\textwidth]{Figures/Model/WebApp.png}
%   \label{fig:webapp}
% \end{figure}


Note that so far, we have only identified cases where spurious group polarization plausibly
occurs in the asymptotic case where $N\to\infty$ ($N$ being the number of participants).
Experimental conditions yielding plausibly spurious group polarization warrant 
some suspicion. If there exists set of latent simple consensus 
parameters $\beta$ that create binned (Equation~\ref{eq:ordinal-frequency})
opinion distributions that naively seem to be group polarized, then the means
of at least some samples from these distibutions with empirical numbers of 
participants would also appear to be shifted consistent with group
polarization. This process itself does not tell us the false detection rate.
We use the identified simple consensus parameters to define pre- and post-deliberation
latent opinion distributions in simulated trials of group polarization 
experiments.


\subsection{Estimating and limiting false detection rates}

To calculate the false detection rate we begin by simulating 
pre- and post-deliberation 
opinions by drawing $N_e$ samples from normal latent opinion
distributions that generated spurious group polarization, i.e., with
parameters labelled $\beta_e$ found in the hillclimbing step. Binning these
random values simulates observations. $N_e$ is the number
of participants in experimental condition $e$. Each trial $i$ of $N_e$
simulated pre- and post-deliberation observations is then inspected to see
if spurious group polarization occurred in the simulation. Spurious group
polarization is determined by first inferring the latent parameters of each
simulated experimental trial by fitting an ordered probit model~\cite{Liddell2018}. 
The inferred parameters are then used to calculate the effect size for that
trial, Cohen's $d$, defined as~\cite[p. 331]{Liddell2018}
\begin{equation}
  d = \frac{\mupost - \mupre}{\sqrt{\frac{1}{2}(\sigmapost^2 + \sigmapre^2)}}.
  \label{eq:cohens}
\end{equation}
\noindent
The family-wise error rate is, $\alpha_{e} = \Pr(D|\neg E)$, is the 
fraction of trials, 
with $d_{ei} > d^*$ for the $i^\text{th}$ trial for experimental
condition $e$, written
\begin{equation}
  \alpha_e = \frac{1}{N_T} \sum_i^{N_T} \Theta(d_{ei}; d^*),
  \label{eq:study_aggregate}
\end{equation}
\noindent
where
\[
  \Theta(d_{ei}; d^*) = 
    \begin{cases}
      1 & \text{ if } d_{ei} > d^* \\
      0 & \text{ otherwise.}
    \end{cases}
\]
and $N_T = 1000$ is the number of trials. For simplicity we assume that the sign of
$d_{ei}$ is positive, meaning a shift to greater extremisim. It is possible that
$d_{ei}$ could be negative, indicating a shift to lesser extremity. Indeed we
observe many simulations with a shift to lesser extremity. For now we do not
include these as indicating a ``significant'' effect, which we return to in the
Analysis below.

To calculate the family-wise error rate
for a study, $s$ which contains several experimental conditions, $e$,
\begin{equation}
  \alpha_s = \frac{1}{|s|} \sum_{e \in s} \alpha_e
\end{equation}
\noindent
The mean across all results identified as plausibly spurious is 
\begin{equation}
  \alpha_\mathrm{all} = \frac{1}{|E|} \sum_{e \in E} \alpha_e.
\end{equation}
We finally may calculate the false discovery rate by plugging this $\alpha$
into Equation~\ref{eq:fdr} and assuming empirically-motivated values for 
base rate, $b$, and statistical power, $W$, as well as more pessimistic and
optimistic settings for base rate and power.

We close our Analysis by inverting this procedure to find what significance value
$d^*$ is necessary to achieve a low family-wise error rate $\alpha_e = 0.05$ for
experimental condition, $e$. To do this we first calculate $\alpha_e(d^*)$ for a
sufficiently wide range of $d^*$. We then find the lowest $d^*$ for which
$\alpha_e(d^*) \leq 0.05$.


\section{Analysis}\label{analysis}

Our analysis estimates how the probability that a published detection of group
polarization is actually a false detection. We first identified latent opinion
parameters that yield spurious group polarization in an asymptotic $N\to\infty$
model. This gives us our first suggestion that experimental conditions may be
plausibly false. Briefly, we identified latent opinion distribution parameters
that generated spurious group polarization in 54 out of 57 experimental
conditions. In other words, 95\% of all group polarization detections are
plausibly spurious due to ceiling artifacts from the continuous latent
opinions-ordinal measurement experimental design. We then analyze the family-wise
error ($\alpha$) and false detection rate (FDR) 
for each experimental condition and aggregated 
up to the study level and across all studies and experimental conditions. 
We estimate the lower twentieth percentile of experimental conditions 
have \emph{at least} $\alpha \leq 0.16$, 
with median $\alpha \geq 0.27$ and upper eightieth percentile with $\alpha \geq
0.40$ (Table~\ref{tab:quantiles}\mt{A.  â€“but don't have A and B; TODO: separate
FWER and FDR tables, only use 0, 20, 50, 80, 100 for probs. Rename 0 -> min, 100 ->
max.}). For the fiftieth and eightieth percentiles this
corresponds to false discovery rates of 0.75 and 0.82, respectively. 
We emphasize that these estimates and others detailed below are lower bounds on
$\alpha$ and FDR since we are using what is likely an overgeneralized,
underspecified statistical model that does not account for all the potential
variance in experimental outcomes, which would increase error rates.
We close our Analysis by calculating which $d^*$ limits family-wise error rates to
$\alpha \leq 0.05$. This is greater than $d^* = 0.8$, classically taken to 
represent ``high'' significance, in all but six experimental conditions.

\subsection{Nearly all group polarization detections are plausibly spurious}

We used our formal distribution model of group polarization experiments
to identify plausible latent simple consensus distributions that generate spurious
group polarization in observed ordinal distributions 
in the asymptotic case of taking $N \to \infty$ ordinal measurements.
Across the ten studies we analyzed, we excluded 3 out of 60 experimental
conditions that did not claim to be positive findings of group polarization.
Of the remaining 57, we identified simple consensus $\beta$ that generate spurious
group polarization in 54 experimental conditions. In the other three, we did
identify $\beta$ that generate spurious group polarization, but the initial
simulated distributions were highly polarized. We rejected $\beta$ generated for
two spurious group polarization from Schkade, et al., (2010) because
the histogram plots of the observed opinion distributions are not
polarized~\cite[Figure 1, p. 234]{Schkade2010}. The other condition where we
found but rejected spurious group polarization $\beta$ was in a question 
about a ``bad teacher'' in Myers (1975)~\cite{Myers1975a}, where lower values
signify increasingly worse impressions of the teacher and higher values signify
increasingly good impressions. It is unlikely that a
``bad teacher'' would be rated as good in this case, so we excluded this 
from our analysis as well. This exclusion may be overly generous since Myers (1975)
did not provide details beyond mean values and opinion shifts, and the article was
published before open data practices.


\subsection{Half of experiments exceed a 70\% false discovery rate at ``medium'' significance}

We simulated trials from each experimental condition, $e$, by sampling and binning
pre- and post-deliberation latent opinions from distributions with parameters
identified with the $N\to\infty$ model as causing plausibly spurious group
polarization. For each $e$, we drew a number of samples equal to the number of
participants in the original experiments, $N_\mathrm{emp}$. To estimate family-wise
error and false detection rates, we fit an ordered probit model to the simulated
observations and obtained estimates of the group polarization effect size, $d$
(Equation~\ref{eq:cohens}). If the effect size was greater than a significance
value, $d > d^*$, then by definition there was a detection $D$ of group
polarization, even though there was no effect by design, written $\neg E$.  The
family-wise error rate is $\Pr(D|\neg E)$, calculated as the frequency with which $d
> d^*$ out of 1000 simulation trials. The false detection rate is the family-wise
error rate scaled using the base rate of effects and the statistical power
(Equation~\ref{eq:fdr}). We test three difference significance values, $d =
0.2,~0.5$ and $0.8$, corresponding to what Cohen prescribed to use for ``low'',
``medium'', and ``high'' significance. If not otherwise noted, we set the base
rate to be $b=0.1$ and the power $W = 0.8$ as metascience parameters to calculate 
the false discovery rate (Equation~\ref{eq:fdr}). Note that if the family-wise error
rate is low at $\alpha = 0.05$, the false discovery rate is $\fdr = 0.36$.

Different conditions yield different effect size distributions
(Figure~\ref{fig:effect_size_distros}), which 
determine the magnitude of error rates and difference between the error rates for low,
medium, and high significance values $d^*=0.2,0.5,0.8$ magnitude of error rates 
(Figure~\ref{fig:fwer_fdr_synthesis}B). 
Family-wise error rates varied from a minimum of one in ten for the
\texttt{Feminists-Experimental} condition from Myers (1975)~\cite{Myers1975a}
to a maximum of two in three for the \texttt{COSprings-CivilUnions} condition from
Schkade, Sunstein, and Hastie (2010)~\cite{Schkade2010} under medium
significance effect size $d^* = 0.5$ (Figure~\ref{fig:fwer_fdr_synthesis} and
Table~\ref{tab:quantiles}). The median family-wise error
rate is 0.28 for Krizan and Baron's (2007) experimental condition
\texttt{NoOutgroupScenario1}~\cite{Krizan2007}. This translated to a minimum
false detection rate of 0.52 for the Myers (1975) condition, a maximum false
detection rate of 0.88 for the Schkade, et al., (2010) condition, and a median false
detection rate of 0.75 for the Krizan and Baron (2007) condition. 

\begin{table}[ht]
  \caption{{\textbf{Quantiles for the two error-rate measures under different
  significance values.}}}
  \label{tab:quantiles}
  \centering
  \input{tex/quantile_table_min_20_50_80_max.tex}
\end{table}

In the conditions
with lower family-wise error rates, there were several conditions where using the low 
significance value $\alpha$ resulted in a much greater error rate than the medium or 
high significance values. In the ten conditions with the lowest $\alpha$ for 
$d^* = 0.5$, with $\alpha$ near 0.1, five of these conditions have $\alpha > 0.25$
when $d^* = 0.2$ corresponding to a small effect size. For low $\alpha$, the
difference in false detection rates for each $d^*$ value are more widely
distributed, following the distribution of $\alpha$, meaning that 
a modest increase in $d^*$ can have greater effects on the false discovery rate. In
the conditions above the 50th percentile, $\alpha$ approaches and exceeds one in
two, even when requiring ``high'' significance with $d^*=0.8$. With $d^* = 0.8$,
$\alpha$ approaches and exceeds one in four, with four near one in two. When 
requiring ``low'' signifance, 

Experimental designs within each study or article tend to share certain
characteristics, such as the number of bins in the ordinal measurement design
or the number of participants per condition. Therefore, the 
average false detection rate across experimental trials within a study
(Equation~\ref{eq:study_aggregate}) show that 

\begin{figure}

  \caption{\textbf{Family-wise error rates and false discovery rates across studies and experiments for
    low, medium, and high Cohen's $d$ significance.}}
  \label{fig:fwer_fdr_synthesis}

  \centering
  \includegraphics[width=1.1\textwidth]{Figures/Analysis/fwer_fdr_synthesis.pdf}
\end{figure} 

\begin{itemize}
  \item 
    Other $b$ and $W$ values in quantiles.
  \item
    Note that there were many ``significant'' results---in the wrong direction
    (Figure~\ref{fig:OrdinalBoxplot}).
\end{itemize}

\subsection{High significance values ($d^*$) often necessary for FWER of 0.05}

Our simulations can be used a different way, in this case to calculate what
significance value $d^*$ limits the family-wise error rate to the low value of
0.05. 
Solving the inverse problem of which $d^*$ achieves a low family-wise error rate,
which drives the false detection rate. Here we present our calculations to find
$d^*$ that achieve $\alpha(d^*) = 0.05$. 


\begin{figure}
  \caption{
    \textbf{Significance value, $d^*$, to limit family-wise error 
    rate of 0.05 for each identified study.} The Moscovici and Zavalloni (1969)
    ``Americans'' question would require the greatest $d^*$ to achieve $\alpha \leq
    0.05$, even though it does not have the highest error rates. This is because of
    larger outliers in simulated $d$ for this condition compared to others with
    greater error rates (Figure~\ref{fig:OrdinalBoxplot}).
  }
  \centering
    \includegraphics[width=0.75\textwidth]{Figures/Analysis/sigval_for_low_fwer.pdf}
  \label{fig:sigval_for_low_fwer}
\end{figure}



\section{Discussion}\label{discussion}

\mt{Restate results and importance and outline this section.}

\mt{One more brick to fall in the ongoing replication and generalizability crisis \autocite{Yarkoni2022}.}

Since there seems to be no selection pressure on false discovery rates, 
experiment design for studying group polarization was free to evolve randomly.
Various choices for the number of bins and other instrumental details may have
then become entrenched in different communities defined by 
common theoretical perspectives taken by researchers in those communities. 
Therefore, when 
Isenberg (1986)~\cite{Isenberg1986} reports that his meta-analysis found more
support for the XX theory than the YY theory in the form of greater effect sizes,
it may just be random cultural drift that the XX community instruments had higher
intrinsic false detection rates. In some cases it may be debatable whether to
control for the Type I error rate, or may depend on context~\cite{Rubin2021}. 
In this context, however, it is clear that we must apply such selection pressure 
to group polarization experiments or it may be nearly impossible to trust 
statistical analyses of these studies.

This leads to one of many follow-up research questions generated by this work.
First, are there general principles of experimental design that could reduce
the false detection rate for group polarization or similar experimental protocols?
Second, what is the best way to calculate false detection rate? 
Likely, even if we cannot identify parameters consistent with simple consensus
that generate spurious group polarization shifts in the large $N$ case, one
may still guess at some simple consensus parameters and caluclate the false
detection rate as we did. On the flip side, we likely found only one of infinitely
many simple consensus parameters that could result in a false detection. Does this
have an effect on the calculated false positive rate? More generally, our
work motivates similar analyses of sub-fields of social 
psychology and that do not account for Likert-based experimental designs in
their statistical analyses. We provided a template for this future work.

Our analysis brings some additional questions into focus. Most significantly, recall that even
the more appropriate ordered probit Bayesian model often failed to accurately detect negative
results with the number of participants used in the original papers. Many of the studies
analyzed here did not use any statistical test to establish significance; those that did used
some form of a $t$-test, and it is not clear if the $t$-test that was used 
accounted for a change in pre- and post-deliberation opinion variance, which we expect due to
consensus.

The ordered probit model we used still likely underestimates the variance in outcomes, which
could require even larger sample sizes. A more rigorous analysis
would include additional sources of variance that will likely further inflate the Type I
error rate, $\alpha$, and the false discovery rate~\cite{Yarkoni2022}. First and foremost,
there is variance within each experimental deliberation group; no group polarization study
accounts for this, instead treating participants in each experimental condition as if they were
one large group. Furthermore, survey data are known to be noisy~\cite{Zaller1992}. People
report opinions differently over time for no apparent reason (i.e., opinions are
\emph{unstable}).  Context matters: the order in which a survey question is asked, and which
question framing is chosen among logically equivalent alternatives, can both significantly
influence participant responses. The effect of accounting for these sources of variance must be
understood to estimate $\alpha$ (and power, $W$) for the design of the next generation of group
polarization experiments.

\mt{concluding paragraph: first sentences summarize the Discussion, then
in 1-2 sentences summarize the paper.}

\printbibliography[title=References]

\appendix

\renewcommand{\thefigure}{A\arabic{figure}}

\setcounter{figure}{0}

\clearpage

%%%%%%%%%% Supplement %%%%%%%%%%
\pagebreak
\begin{center}
  \textbf{\Large \textsf{Group polarization replications may be marred by high false discovery rates (Supplementary Material)}}
\end{center}

%%%%%%%%%% Prefix a "S" to all equations, figures, tables and reset the counter %%%%%%%%%%
\setcounter{equation}{1}
\setcounter{figure}{0}
\setcounter{section}{0}
\setcounter{table}{0}
\setcounter{page}{1}
\makeatletter
\renewcommand{\theequation}{S\arabic{equation}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\thesection}{S\arabic{section}}
\renewcommand{\thepage}{S\arabic{page}}

\section{Analysis}

\begin{figure}
  \caption{Boxplot of Cohen's $d$ for the ordered probit model over 1000
    simulation trias for each experimental condition (y-axis in A and B). Each
    trial represents a possible outcome of a group polarization experiment where
    the true opinion shift is zero. The mean across trials is closer to 0 than
    in the metric case, but many simulated zero-shift experiments still result in
    an observed shift. This illustrates the weak statistical power of these group
    polarization experiments.}
  \label{fig:OrdinalBoxplot}
  \centering
    \includegraphics[width=1.0\textwidth]{Figures/Analysis/ordinal_cohens.pdf}
\end{figure}


\end{document}
