% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  abstract]{article}

\usepackage{floatrow}
\DeclareFloatFont{small}{\small}% "scriptsize" is defined by floatrow, "tiny" not
\floatsetup[table]{font=small}

\usepackage[table]{xcolor}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{array}

\usepackage{amsmath,amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{iftex}
\input{preamble.tex}
\hypersetup{
  pdftitle={Group polarization replications may be marred by high false discovery rates},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue}}


\newcommand{\mupre}{\mu_\mathrm{pre}}
\newcommand{\mupost}{\mu_\mathrm{post}}

\newcommand{\sigmapre}{\sigma_\mathrm{pre}}
\newcommand{\sigmapost}{\sigma_\mathrm{post}}

\newcommand{\thetafit}{\theta^\mathrm{fit}}

\newcommand{\fdr}{\mathrm{FDR}}

\newcommand{\meanobs}{\bar{o}}
\newcommand{\meanobsemp}{\bar{\mathbf{o}}}
\newcommand{\meanobst}{\meanobs_t}
\newcommand{\meanobsjt}{\meanobs_{j,t}}

\newcommand{\meanobspre}{\meanobs_\mathrm{pre}}
\newcommand{\meanobspost}{\meanobs_\mathrm{post}}

\definecolor{myorange}{RGB}{240, 96, 0}
\newcommand{\mt}[1]{{\textcolor{myorange} {({\tiny MT:} #1)}}}

\usepackage{authblk}
\usepackage{etoolbox}
\usepackage{wrapfig}
\makeatletter
% \title{If the Null Fits, You Must Omit: Ubiquitous False Detections of Group Polarization}
\title{If the Null Fits, You Must Omit: Measurement Artifacts Undermine Group Polarization Replications}
\makeatother
\author[1,*]{{Matthew A.~Turner}}
\affil[1]{\small Environmental Social Sciences, Stanford Doerr School of Sustainability, Stanford University}

\author[2,3]{{Paul E.~Smaldino}}
\affil[2]{\small Cognitive and Information Sciences, University of California, Merced}
\affil[3]{\small Santa Fe Institute} 
% \vspace{2em}
\affil[*]{\small Correspondence: \href{mailto:maturner@stanford.edu}{maturner@stanford.edu}}

\date{\today}
\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=gray}
\setcounter{tocdepth}{2}
\tableofcontents
}

\begin{center} \noindent\rule{4cm}{0.4pt} \end{center}
% \clearpage

\begin{quote}
In our introductory social psychology course, 
we have for many years used the [group polarization experimental paradigm] as
a laboratory exercise. The exercise works beautifully, but one must be
careful to forewarn a class that [group polarization] does not occur with every 
group\ldots and that the effect is not large. 
\par\raggedleft(Brown, 1986, p.\cite[p. 205]{Brown1986})
\end{quote}

\begin{quote}
  The good thing about science is that it's true, whether or not you believe
  it. (Neil de Grasse Tyson slogan circa 2018)
\end{quote}


\input{intro-latest.tex}

% \input{old-intro.tex}

\section{Methods}\label{methods}

To find examples of plausibly spurious group polarization we guess at what latent
mean, combined with a latent pre- and post-deliberation latent standard deviations,
could generate it. We calculate the observed group polarization for these latent
parameters by transforming the latent pre- and post-deliberation distributions from
continuous normal distributions to ordinal ``observations'' by integrating over each
ordinal bin, then calculating the observed mean for each experiment \emph{phase},
i.e., both the pre- and post-deliberation phases.  If the null model can generate
group polarization in this case, then the detection is labelled \emph{plausibly
false}, i.e., \emph{plausibly spurious}, and we call the generating latent
parameters the \emph{plausible null parameters}. 

To evaluate whether the plausible null parameters we identify could realistically
fool a significance test, we implemented a generative simulation of each plausibly
false detection of group polarization and calculated the \emph{family-wise error rate
(FWER)} and the \emph{false discovery rate (FDR)}. We use the published number of
participants $N$ for each of the 54 conditions that yielded plausibly false
detections of group polarization. The FWER is also known as the Type I
error, or the false positive rate: it is the probability that a detection of an
effect occurs given that in reality there was a null effect.  The FDR is the
probability that a published positive detection of an effect is a false positive,
i.e., a detection of a null effect. High FWER and FDR for an experimental condition
indicates that its design is weak, since we identify only one of perhaps infinitely
many latent parameter combinations that can generate spurious group polarization. 

We use this setup to demonstrate how \emph{adversarial} style testing of experimental
designs with counterfactual null models can be used to estimate important
metascience parameters. In this study we use our FWER calculations to estimate
the minimum Cohen's $d$ that should be used to determine whether an effect is
``significant'' or not that will limit the FWER to 5\%. In the Discussion we
sketch how our model can then be extended to rigorously calculate statistical power,
i.e., the probability that a significant effect is detected given that there 
is indeed a significant effect.


\subsection{Probability theory of spurious group polarization}

We formalize this experimental design in the $N\to\infty$ case where the
distribution of ordinal measurements is created by integrating continuous latent
opinion distributions.  Latent opinions in the model are normally distributed with
mean $\mu_t$ and variance $\sigma_t$ for time $t$. We are concerned with only two
times, \emph{pre-} and \emph{post-} deliberation ($t=0$ and $t=T$, respectively).
For short we can write $\mu_\mathrm{pre}$ and $\sigma_\mathrm{pre}$ for
pre-deliberation mean and variance (and similar for \emph{post}-deliberation).  
The latent opinion of participant $i$ at time $t$ is drawn from a normal 
distribution in the model, 
\begin{equation} 
  o_{i,t} \sim p(o; \mu_t, \sigma_t) = \mathcal{N}(\mu_t, \sigma_t).
  \label{eq:opinionDistribution} 
\end{equation} 
\noindent 
Latent opinions are assumed to be normally distributed since they are unbounded 
aggregations of neurobiological activity. 

The measurement process with $N\to\infty$ participants is simulated by 
integrating the latent opinion distribution between ordinal bin thresholds, 
of which there are $B+1$, where $B$ is the number of opinion bins, with sequential
integer values $b$. In the Moscovici and Zavalloni (1969) example, $b \in
\{-3,-2,\ldots,2,3\}$ and $B = 7$.
Bin $b$ is defined by its lower and upper continuous-valued thresholds, 
$\theta_{b-1}$ and
$\theta_{b}$, respectively. Thresholds are given by the following equations
(and visualized as dotted lines in Figure~\ref{fig:distros}).
\begin{equation}
\theta_b = \begin{cases}
  -\infty         & \text{ if } b=0 \\
  \infty          & \text{ if } b=B \\
  b + \frac{1}{2} & \text{ otherwise.}
\end{cases}
\end{equation}
\noindent
With thresholds defined, the frequency of ordinal measurment $b$ at time $t$ 
is (Figure~\ref{fig:distros})
\begin{equation}
  p(b;~\mu_t, \sigma_t, B) = \int_{\theta_{b-1}}^{\theta_b} p(o; \mu_t, \sigma_t)
  do.
  \label{eq:ordinal-frequency}
\end{equation}

We can now define spurious group polarization and give examples of when it occurs.
First, the observed group polarization opinion shift is
\begin{equation}
  g = \meanobs_\mathrm{post} - \meanobs_\mathrm{pre}~,
  \label{eq:group-polarization}
\end{equation}
\noindent
where the expected value of observed opinions at time $t$ is
\begin{equation}
  \meanobst = \frac{1}{B} \sum_{b=1}^B b \cdot p(b; \mu_t, \sigma_t, B).
  \label{eq:meanobs}
\end{equation}
\noindent

\emph{Spurious} group polarization is when mean observed opinions seem to increase in 
extremity when the latent mean is held constant. Call the collection of
latent null model distribution parameters $\lambda = (\mu, \sigma_{pre}, \sigma_{post})$.
The group polarization opinion shift can then be written as a function of $\lambda$,
$g(\lambda)$. Formally, then, spurious group polarization is plausible if 
there exists at least one $\lambda$ for which $g(\lambda) = g_{pub} > 0$, where
$g_{pub}$ is the published group polarization shift value.
This could be thought of as sampling the observed ordinal distributions 
$N \to \infty$, collecting ordinal measurements of samples from 
each latent distribution corresponding to each phase, pre- and post-deliberation.

Next, we explain how to find a set of latent null parameters, $\lambda$, that
generate spurious group polarization when naively measured on an ordinal scale.

\begin{figure}
  \caption{\textbf{Opinion measurement model and spurious group polarization.} 
  Latent model participant opinions are drawn from a normal distribution with the
  listed parameters (A,C). The pre-deliberation latent distribution has high
  variance, but appears polarized when measured via integration (A).  The latent
  distribution's variance is expected to decrease during deliberation (B) via
  consensus, but in this example the latent mean is static, 
$\mupre = \mupost$. However, when the post-deliberation distribution is measured,
the simulated observed mean increases, i.e., spurious
group polarization occurs (C).}
  \centering
  \includegraphics[width=0.95\textwidth]{Figures/Model/latent-ordinal-distros.pdf}
  \label{fig:distros}
\end{figure}

\subsection{Interactive algorithm to find parameters inducing spurious group polarization} 

To identify plausibly false detections of group polarization, we searched numerically
for latent parameters $\lambda = (\mu, \sigmapre, and \sigmapost)$ that generate
spurious group polarization, as explained immediately above. We developed an *ad hoc*
hillclimbing-style algorithm that finds $\lambda$ that finds $\lambda$ by minimizing
the difference between published group polarization shifts $g_pub$ and calculated shifts,
$g_\lambda$.
calculated with Equation~\ref{eq:meanobs}. 

The algorithm to find $\lambda$ holds $\mu$ constant while searching for $\sigmapre$
and $\sigmapost$ that generate observed $\meanobspre$ and $\meanobspost$, within a
tolerance of $\pm10^{-3}$ (Algorithm~\ref{alg:hillclimbing}). 
The algorithm requires a researcher to specify an initial
guess for $\lambda$ and auxiliary parameter settings, which generally vary between
experimental conditions.  Furthermore, the algorithm does not always produce
reasonable distributions with a given $\lambda$, and sometimes it does not produce
spurious group polarization.  Therefore, it is necessary to sanity check the latent
and ordinal opinion distributions for the $\lambda$ found by the algorithm. If all is
well, the experimental condition can be marked as plausibly spurious.  To tame the
complexity involved in data entry and model fitting for each of these sixty
experimental conditions, we developed a web application to input data from control
the algorithm and inspect its results (Figure~\ref{fig:webapp}). Web application code is
available online in our GitHub
repository\footnote{\url{https://github.com/mt-digital/gp-statmod/blob/main/GroupPolarizationStatmod/app.R}}.

\begin{algorithm}
  \caption{Routine to find latent mean and variance that generate spurious group polarization.}
  \label{alg:hillclimbing}
  \begin{algorithmic}

    \Require $\mu$, $\sigma$, $\bar{\mathbf{o}}$ 
      \Comment{Set latent mean, guess variance that yields, input empirical mean.}

    \Require $\epsilon$, $\delta$, $i_\mathrm{max}$ 
      \Comment{Hillclimbing tolerance, step size, and maximum iterations.}

    \State $\Delta \gets \meanobs(\mu, \sigma) - \meanobsemp$ 
      \Comment{Initialize simulation error.}
    \While {$\Delta^2 > \epsilon$ and $i < i_\mathrm{max}$} \Comment{Search until desired
    accuracy or max iterations reached.}
      \State $\sigma' \sim \mathcal{N}(\sigma, \delta)$ \Comment{Draw new variance to test.}
      \State $\Delta' \gets \bar{o}(\mu, \sigma') - \bar{\mathbf{o}}$ 
        \Comment{Calculate error using new variance.}
      \If {$\Delta' - \Delta < 0$}
        \State $\sigma \gets \sigma'$
          \Comment{Update $\sigma$ if the error is less than before.}
        \State $\Delta \gets \bar{o}(\mu, \sigma) - \bar{\mathbf{o}}$
          \Comment{Update simulation error if $\sigma$ is updated.}
      \EndIf
    \EndWhile \\

    \Return $\sigma$
  \end{algorithmic}
\end{algorithm}


% \begin{figure}
%   \caption{\textbf{Shiny web application for user-guided hillclimbing to find
%   parameters that generate spurious group polarization.} This web application enables users to
% create new analyses of experimental conditions from published studies. Users 
% provide initial guesses for the algorithm to use as starting points in 
% finding variances that generate spurious group polarization. Users inspect 
% model fits provided by the algorithm, adjusting parameters if necessary, and
% mark an experimental condition as ``Plausibly spurious'' if a reasonable fit
% is indeed found.\mt{Maybe add some A, B, C guides to the figure and include
% in parentheses here to go with the text?}}
%   \centering
%     \includegraphics[width=0.75\textwidth]{Figures/Model/WebApp.png}
%   \label{fig:webapp}
% \end{figure}


Note that so far, we have only identified cases where spurious group polarization plausibly
occurs in the asymptotic case where $N\to\infty$ ($N$ being the number of participants).
Experimental conditions yielding plausibly spurious group polarization warrant 
some suspicion. If there exists set of latent simple consensus 
parameters $\lambda$ that create binned (Equation~\ref{eq:ordinal-frequency})
opinion distributions that naively seem to be group polarized, then the means
of at least some samples from these distibutions with empirical numbers of 
participants would also appear to be shifted consistent with group
polarization. This process itself does not tell us the false detection rate.
We use the identified simple consensus parameters to define pre- and post-deliberation
latent opinion distributions in simulated trials of group polarization 
experiments.

\begin{figure}
  \centering
    % \includegraphics[width=0.8\textwidth]{Figures/Model/data-model-flow.png}
    \includegraphics[width=\textwidth]{Figures/Model/data-model-flow-w-webapp.pdf}
  \caption{\textbf{Workflow of the GP-stats simulation and analysis pipeline
  for estimating false detection rates under null-consistent conditions.} Colored boxes indicate file types: teal = R scripts, blue = data files, orange = plotting scripts, cyan
= Slurm batch script. Arrows show the flow from parameter estimation (hillclimbing or Shiny app) to Bayesian null model fitting on the cluster, aggregation of simulated results,
and final visualization. (D) shows the web app interface that takes researcher guesses at
simple consensus models that generate observations, and the app passes the guess to initialize
the hillclimbing search algorithm for finding spurious group polarization.}
  \label{fig:data-model-flow.png}
\end{figure}



\subsection{Metascience of the false detection rate in group polarization}

Metascience provides a framework for
calculating the false discovery rate across experimental conditions,
aggregated to the level of journal articles and the ten-paper corpus. 
The false discovery rate is then the Type I error rate relative to the rate of
positive discoveries, true or false.
We calculate via simulation and data fitting
the Type I error rate for the experimental designs used in experimental conditions
that, under a preliminary large-$N$ model analysis, generated group
polarization upon ordinal measurement of simple consensus latent opinion 
distributions. These can then be aggregated to estimate the false discovery rate
for each journal article and for the ten-article corpus, which is representative of the
false discovery rate for group polarization studies. 

The false detection rate is the scaled probability that a detection, $D$, of
an effect, $E$, is in fact a false detection and there is \emph{not} an effect,
denoted $\neg E$. The probability that a detection $D$ has occurred when there
was no effect, i.e. when $\neg E$, is denoted $\alpha = \Pr(D | \neg E)$.
This is known as the Type I or family-wise error rate. To calculate the 
false detection rate ($\fdr$), one must compare $\alpha$ to the statistical power,
the probability of a detection when there is an effect, denoted $W = \Pr(D | E)$.
Both these values must be scaled by the rate at which effects occur, denoted $b$.
Using the notation outlined above, the false detection rate is
\begin{equation}
  \fdr = \frac{(1 - b)\alpha}{(1 - b)\alpha + bW}.
  \label{eq:fdr}
\end{equation}

In group polarization, $E$ represents the group polarization \emph{effect}, and
$\neg E$ represents its absence. Simple consensus is one way for group polarization
to be absent, i.e., simple consensus is one form of $\neg E$. The presence of
an effect $E$ is independent of whether an effect was \emph{detected}, 
represented by $D$ if it was detected and $\neg D$ if not. 

The group polarization studies are journal articles that present
the results of multiple experiments to support some hypothesis. To organize
the presentation of false detection rates across the ten studies in our
analysis, we first will organize false detection rates by experiment, where
we created `ExperimentID` tags for each of the sixty experiments across ten
studies. We also created `StudyID` tags for each of the ten journal articles
with the key `{Author}{Year}`, e.g., \texttt{Schkade2010} for Schkade, et al.,
(2010) \cite{Schkade2010}. 

We calculate $\alpha = \Pr(D | \neg E)$ through simulation for experimental
conditions, 
and use optimistic values of
$b=0.1$~\cite{Ioannidis2005,McElreath2015} and $W=0.8$~\cite{Smaldino2016NatSel}
in the main text and in the compare with more pessimistic, yet realistic, estimates
of these values, as well as even more optimistic estimates. 



\subsection{Estimating and limiting false detection rates}

To calculate the false detection rate we begin by simulating 
pre- and post-deliberation 
opinions by drawing $N_e$ samples from normal latent opinion
distributions that generated spurious group polarization, i.e., with
parameters labelled $\lambda_e$ found in the hillclimbing step. Binning these
random values simulates observations. $N_e$ is the number
of participants in experimental condition $e$. Each trial $i$ of $N_e$
simulated pre- and post-deliberation observations is then inspected to see
if spurious group polarization occurred in the simulation. Spurious group
polarization is determined by first inferring the latent parameters of each
simulated experimental trial by fitting an ordered probit model~\cite{Liddell2018}. 
The inferred parameters are then used to calculate the effect size for that
trial, Cohen's $d$, defined as~\cite[p. 331]{Liddell2018}
\begin{equation}
  d = \frac{\mupost - \mupre}{\sqrt{\frac{1}{2}(\sigmapost^2 + \sigmapre^2)}}.
  \label{eq:cohens}
\end{equation}
\noindent
The family-wise error rate is, $\alpha_{e} = \Pr(D|\neg E)$, is the 
fraction of trials, 
with $d_{ei} > d^*$ for the $i^\text{th}$ trial for experimental
condition $e$, written
\begin{equation}
  \alpha_e = \frac{1}{N_T} \sum_i^{N_T} \Theta(d_{ei}; d^*),
  \label{eq:study_aggregate}
\end{equation}
\noindent
where
\[
  \Theta(d_{ei}; d^*) = 
    \begin{cases}
      1 & \text{ if } d_{ei} > d^* \\
      0 & \text{ otherwise.}
    \end{cases}
\]
and $N_T = 1000$ is the number of trials. For simplicity we assume that the sign of
$d_{ei}$ is positive, meaning a shift to greater extremisim. It is possible that
$d_{ei}$ could be negative, indicating a shift to lesser extremity. Indeed we
observe many simulations with a shift to lesser extremity. For now we do not
include these as indicating a ``significant'' effect, which we return to in the
Analysis below.

To calculate the family-wise error rate
for a study, $s$ which contains several experimental conditions, $e$,
\begin{equation}
  \alpha_s = \frac{1}{|s|} \sum_{e \in s} \alpha_e
\end{equation}
\noindent
The mean across all results identified as plausibly spurious is 
\begin{equation}
  \alpha_\mathrm{all} = \frac{1}{|E|} \sum_{e \in E} \alpha_e.
\end{equation}
We finally may calculate the false discovery rate by plugging this $\alpha$
into Equation~\ref{eq:fdr} and assuming empirically-motivated values for 
base rate, $b$, and statistical power, $W$, as well as more pessimistic and
optimistic settings for base rate and power.

We close our Analysis by inverting this procedure to find what significance value
$d^*$ is necessary to achieve a low family-wise error rate $\alpha_e = 0.05$ for
experimental condition, $e$. To do this we first calculate $\alpha_e(d^*)$ for a
sufficiently wide range of $d^*$. We then find the lowest $d^*$ for which
$\alpha_e(d^*) \leq 0.05$.


\section{Analysis}\label{analysis}

We now present our determinations of whether published detections of group
polarization are plausibly spurious and calculate the best-case false detection
rate for each experimental design in the corpus. We found that 95\% of all
detections of group polarization are plausibly false, 54 out of 57. We estimate the
best-case median false detection rate to be 72\% for those 54 experimental
conditions. All but seven of the 54 designs were found to require a $d^* > 0.8$
for ``significant'' Cohen's $d$ to limit the family-wise error rate to 5\%.


\subsection{Nearly all group polarization detections are plausibly spurious}


% We used our formal distribution model of group polarization experiments
% to identify plausible latent simple consensus distributions that generate spurious
% group polarization in 
Of the remaining 57, we identified simple consensus $\lambda$ that generate spurious
group polarization in 54 experimental conditions. In the other three, we did
identify $\lambda$ that generate spurious group polarization, but the initial
simulated distributions were highly polarized. 
We rejected $\lambda$ generated for
two spurious group polarization from Schkade, et al., (2010) because
the histogram plots of the observed opinion distributions are not
polarized~\cite[Figure 1, p. 234]{Schkade2010}. The other condition where we
found but rejected spurious group polarization $\lambda$ was in a question 
about a ``bad teacher'' in Myers (1975)~\cite{Myers1975a}, where lower values
signify increasingly worse impressions of the teacher and higher values signify
increasingly good impressions. It is unlikely that a
``bad teacher'' would be rated as good in this case, so we excluded this 
from our analysis as well. This exclusion may be overly generous since Myers (1975)
did not provide details beyond mean values and opinion shifts, and the article was
published before open data practices. Our corpus had a total of 60 experimental conditions, but we excluded 3 out of 60 
that did not claim to be positive findings of group polarization.

- Address sigmas, note differences

\begin{figure}
  \centering
    \includegraphics[width=\textwidth]{Figures/3.1/latent-sd-cdf.png}
    \caption{\textbf{}}
  \label{fig:}
\end{figure}



\subsection{Half of experiments exceed a 70\% false discovery rate at ``medium'' significance}

We simulated trials from each experimental condition, $e$, by sampling and binning
pre- and post-deliberation latent opinions from distributions with parameters
identified with the $N\to\infty$ model as causing plausibly spurious group
polarization. For each $e$, we drew a number of samples equal to the number of
participants in the original experiments, $N_\mathrm{emp}$. To estimate family-wise
error and false detection rates, we fit an ordered probit model to the simulated
observations and obtained estimates of the group polarization effect size, $d$
(Equation~\ref{eq:cohens}). If the effect size was greater than a significance
value, $d > d^*$, then by definition there was a detection $D$ of group
polarization, even though there was no effect by design, written $\neg E$.  The
family-wise error rate is $\Pr(D|\neg E)$, calculated as the frequency with which $d
> d^*$ out of 1000 simulation trials. The false detection rate is the family-wise
error rate scaled using the base rate of effects and the statistical power
(Equation~\ref{eq:fdr}). We test three difference significance values, $d =
0.2,~0.5$ and $0.8$, corresponding to what Cohen prescribed to use for ``low'',
``medium'', and ``high'' significance. If not otherwise noted, we set the base
rate to be $b=0.1$ and the power $W = 0.8$ as metascience parameters to calculate 
the false discovery rate (Equation~\ref{eq:fdr}). Note that if the family-wise error
rate is low at $\alpha = 0.05$, the false discovery rate is $\fdr = 0.36$.

Different conditions yield different effect size distributions
(Figure~\ref{fig:effect_size_distros}), which 
determine the magnitude of error rates and difference between the error rates for low,
medium, and high significance values $d^*=0.2,0.5,0.8$ magnitude of error rates 
(Figure~\ref{fig:fwer_fdr_synthesis}B). 
Family-wise error rates varied from a minimum of one in ten for the
\texttt{Feminists-Experimental} condition from Myers (1975)~\cite{Myers1975a}
to a maximum of two in three for the \texttt{COSprings-CivilUnions} condition from
Schkade, Sunstein, and Hastie (2010)~\cite{Schkade2010} under medium
significance effect size $d^* = 0.5$ (Figure~\ref{fig:fwer_fdr_synthesis} and
Table~\ref{tab:quantiles}). The median family-wise error
rate is 0.28 for Krizan and Baron's (2007) experimental condition
\texttt{NoOutgroupScenario1}~\cite{Krizan2007}. This translated to a minimum
false detection rate of 0.52 for the Myers (1975) condition, a maximum false
detection rate of 0.88 for the Schkade, et al., (2010) condition, and a median false
detection rate of 0.75 for the Krizan and Baron (2007) condition. 

\begin{table}[ht]
  \caption{{\textbf{Quantiles for the two error-rate measures under different
  significance values.}}}
  \label{tab:quantiles}
  \centering
  \input{tex/quantile_table_min_20_50_80_max.tex}
\end{table}

In the conditions
with lower family-wise error rates, there were several conditions where using the low 
significance value $\alpha$ resulted in a much greater error rate than the medium or 
high significance values. In the ten conditions with the lowest $\alpha$ for 
$d^* = 0.5$, with $\alpha$ near 0.1, five of these conditions have $\alpha > 0.25$
when $d^* = 0.2$ corresponding to a small effect size. For low $\alpha$, the
difference in false detection rates for each $d^*$ value are more widely
distributed, following the distribution of $\alpha$, meaning that 
a modest increase in $d^*$ can have greater effects on the false discovery rate. In
the conditions above the 50th percentile, $\alpha$ approaches and exceeds one in
two, even when requiring ``high'' significance with $d^*=0.8$. With $d^* = 0.8$,
$\alpha$ approaches and exceeds one in four, with four near one in two. When 
requiring ``low'' signifance, 

Experimental designs within each study or article tend to share certain
characteristics, such as the number of bins in the ordinal measurement design
or the number of participants per condition. Therefore, the 
average false detection rate across experimental trials within a study
(Equation~\ref{eq:study_aggregate}) show that 

\begin{figure}

  \caption{\textbf{Family-wise error rates and false discovery rates across studies and experiments for
    low, medium, and high Cohen's $d$ significance.}}
  \label{fig:fwer_fdr_synthesis}

  \centering
  \includegraphics[width=1.1\textwidth]{Figures/Analysis/fwer_fdr_synthesis.pdf}
\end{figure} 

\begin{itemize}
  \item 
    Other $b$ and $W$ values in quantiles.
  \item
    Note that there were many ``significant'' results---in the wrong direction
    (Figure~\ref{fig:OrdinalBoxplot}).
\end{itemize}

\subsection{High significance values ($d^*$) often necessary for FWER of 0.05}

Our simulations can be used a different way, in this case to calculate what
significance value $d^*$ limits the family-wise error rate to the low value of
0.05. 
Solving the inverse problem of which $d^*$ achieves a low family-wise error rate,
which drives the false detection rate. Here we present our calculations to find
$d^*$ that achieve $\alpha(d^*) = 0.05$. 


\begin{figure}
  \caption{
    \textbf{Significance value, $d^*$, to limit family-wise error 
    rate of 0.05 for each identified study.} The Moscovici and Zavalloni (1969)
    ``Americans'' question would require the greatest $d^*$ to achieve $\alpha \leq
    0.05$, even though it does not have the highest error rates. This is because of
    larger outliers in simulated $d$ for this condition compared to others with
    greater error rates (Figure~\ref{fig:OrdinalBoxplot}).
  }
  \centering
    \includegraphics[width=0.75\textwidth]{Figures/Analysis/sigval_for_low_fwer.pdf}
  \label{fig:sigval_for_low_fwer}
\end{figure}



\section{Discussion}\label{discussion}

\mt{Restate results and importance and outline this section.}

Since there seems to be no selection pressure on false discovery rates,
experiment design for studying group polarization was free to evolve
randomly. Various choices for the number of bins and other instrumental
details may have then become entrenched in different communities defined by
common theoretical perspectives taken by researchers in those communities.
Therefore, when Isenberg (1986)~\cite{Isenberg1986} reports that his
meta-analysis found more support for the persuasive arguments theory than the 
social comparisons theory in the form of greater effect sizes. If this were
indeed the case, perhaps it is only because some researchers hypothesizing
persuasive arguments happened to use a measurement procedure produced more
frequent and extreme distortions of opinion measurements. Incentive
structures known to promote the ``evolution of bad science'' would select
for research that ignored sticky, time-consuming statistical problems in
favor of juicy titles and headlines for which practitioners have won several
professional awards. Specifically, the traditional bias
towards positive results, and against negative ones, could easily have
provided a cultural niche in which quick but faulty group polarization
statistical methods dominate slower, more rigorous ones.
Now that we are aware of the problem, there must be
intense selective pressure applied as soon and as widely as possible to 
avoid further ``replications'' that report spurious group polarization, or
any similar latent psychological dynamics. 

We provided tools that experimenters can use. While these tools are
potentially useful, they are still in the prototype phase that was
sufficient for a single researcher who was also the software developer. So,
a first step towards expanding the usefulness of the findings here is to 
improve the software usability and make the app widely available for
researchers in group polarization and beyond. Another step would be to go
beyond reactionary testing of the reliability of experimental designs, and
identify design principles that reduce the false detection rate without
needing to adjust the Cohen's $d$ (or whatever measure) 
used as a significance threshold. 

When we used the appropriate ordered probit Bayesian statistical model, it
still often failed to accurately identify mere agreement, instead detecting
group polarization.  The ordered probit model we used still likely
underestimates the variance in outcomes, which could require even larger
sample sizes. A more rigorous analysis would include additional sources of
variance that will likely further inflate the Type I error rate, $\alpha$,
and the false discovery rate~\cite{Yarkoni2022}. First and foremost, there is
variance within each experimental deliberation group; no group polarization
study accounts for this, instead treating participants in each experimental
condition as if they were one large group. Furthermore, survey data are known
to be noisy~\cite{Zaller1992}. People report opinions differently over time
for no apparent reason (i.e., opinions are \emph{unstable}).  Context
matters: the order in which a survey question is asked, and which question
framing is chosen among logically equivalent alternatives, can both
significantly influence participant responses. Finally, little is known about
the psychological process that converts latent opinions to reporting
behaviors (e.g., clicking a radio button corresponding to an opinion).  The
effect of accounting for these sources of variance must be understood to
estimate $\alpha$ (and power, $W$) for the design of the next generation of
group polarization experiments. 

A simple change in measurement procedure could fix the problem identified
here: center the post-discussion scale on each group’s own pre-discussion
mean. This removes the assumption that all groups share a common “neutral”
midpoint. The situation is like tossing a ball on a moving train: to the
passengers, the ball’s speed is the same in both directions, but to an
observer on the ground one throw appears faster and the other slower.
Likewise, when measurement is anchored to an external scale, simple agreement
within a group can appear as movement toward one pole. Re-centering the
measurement frame corrects this distortion. It acknowledges that opinion
change is relative to the observer’s reference frame—there is no fixed
background of neutrality against which all groups can be compared. This is,
in essence, the insight of Newtonian relativity: apparent motion depends on
the frame from which it is measured.

First we must note that there are even more, perhaps even graver problems
with group polarization research than I discussed so far: one of note is
opinion instability. Opinions are not generally stable—people seem to
construct their opinions based on several factors not directly related to
whatever underlying topic. Factors affecting reported opinions include the
language used to frame a prompt or question; the order of questions on a
multi-item survey; or simply when the question was asked (Kalton and Schuman,
1982; Zaller and Feldman, 1992; Zaller 1992). Failure to account for sources
of input variance in any experiment can lead to inflated effect sizes since
the input variance is missing from the posterior predictions during
statistical model fitting (Yarkoni, 2022). This further compounds the
theoretical confusions that fill the void when theories are never formalized
and grounded in a mathematical or computational model (Smaldino 2020; Turner
and Smaldino, 2022).

A second additional problem with group polarization research is that we literally cannot learn anything by comparing summary statistics obtained with incommensurate experimental designs—but this is exactly how group polarization research has tried to make progress. Comparing summary statistics only has explanatory power if datasets were generated under the same experimental designs—it seems this was never done or even considered in the group polarization literature. In order to compare studies, their datasets must be generated from commensurate experiments, meaning the studies must share the same predictive and statistical models and experimental design. Otherwise, such comparisons are meaningless, they literally tell us nothing, as shown by philosopher of science Nancy Cartwright in Chapter 5 of her book The Dappled World: A Study of the Boundaries of Science.


\subsection{Humble Hearts Promote Rigorous Science}

Cass Sunstein kicked off the 2000s by elevating group polarization to be a scientific law and advocating the application of group polarization research to serious real-world problems in two books (Sunstein, 2009; Sunstein, 2019) and two recent high-profile articles in Nature Human Behavior: one on the social foundations of pandemic preparedness (Van Bavel, et al., 2020) and one to “promote truth, autonomy, and democratic discourse online” (Lorenz-Spreen, Lewandowski, Sunstein, and Hertwig, 2020). Let us try to stop the spread of this bad science—and let us explore what must be done to achieve Dorian Cartwright’s vision of a rigorous science of group polarization he articulated fifty years ago.

To be practically useful, science must be rigorous. Rigorous science comes most naturally when scientists remain humble, especially when they serve at the pleasure of the public, the government, or other donors. Here’s another example from Burnstein and Vinokur (1973) that we can learn from—let’s try to avoid the sort of arrogance and haughtiness that seep from these acknowledgements they wrote on p. 123:

\begin{quote}
This research was supported by a Grant from the National Institute of Mental Health (MH-16950-03) and by a Guggenheim Foundation Fellowship awarded to the first author. We gratefully acknowledge the incisive, witty, and vinous commentary provided by colleagues at the University of Provence, in particular Robert Abelson, Claude Flament, and Jean-Pierre Poitou, as well as the spirited and intelligent efforts of Livia Mezrick and Irene Graczyk, who helped us carry out the study.
\end{quote}

Science is not glamorous—it takes guts and sacrifice to achieve the technical skills and discipline to sharpen one’s prose, methods, mathematics, and computer code to the point they can really cut reality at its joints, investigate different parts, then put it all back together with a message for others about how the world really works. These days, to do rigorous social science work one must wisely curate the best of the available social science by harmonizing cross-disciplinary jargon. But disincentives still exist against careful scholarship. Prizes and professorships go to those with the most numerous publications in the right journals—period.

Unpaid, anonymous reviewers are supposed to act as gatekeepers to catch problems like the ones I reviewed. But, journal article reviewers have little to gain from enforcing rigor—actually there is a lazy strategy that can pay off big for freeloaders. It’s easy to imagine a cultural norm evolving where reviewers save themselves time and energy by asking little of an author, in hopes that little will be demanded of them when the reviewer submits their next paper. This could result in a publication process that rewards the laziest and most willing to flood the zone and self-aggrandize.

\subsection{Conclusion}

Social science can use physics as a model discipline to transition from idiosyncratic and Balkanized to informative and united. Physics only changed the world because it is useful—it is useful only because it reliably predicts things. Physics became more useful as it became more united, finding shared representations for concepts developed independently—for example the science of optics was developed independently from the science of dynamics, but the physics of light is explained partly in analogy to a pendulum or spring—the mathematics of harmonic motion can be used to predict the motion of springs or pendulums—or why we see different colors of light through a prism. With careful, coordinated tweaks to group polarization experimental design and theoretical modeling, we will build a more rigorous theory of group polarization and its large-scale expression, echo chamber radicalization.

I expect that group polarization experiments, when properly constructed and analyzed, will help sustainability campaigns optimize their social influence campaigns. We can start by developing simpler experimental designs based on formal or computational models capable of predicting outcomes in an experiment. I have developed a model that could motivate an experimental design: I showed that if extremists are more stubborn, this is sufficient to cause group polarization when likeminded groups interact over time. There is even a free stubborn extremism parameter that could be fit to data to measure the relationship between stubbornness and extremism on different issues.

Group polarization research so far has mostly provided us with a checklist of what not to do. The alternative approach I outline builds on rock—the rock of rigor—and will supersede past group polarization research built on a thousand grains of inexact verbal speculation.

\printbibliography[title=References]

\appendix

\renewcommand{\thefigure}{A\arabic{figure}}

\setcounter{figure}{0}

\clearpage

%%%%%%%%%% Supplement %%%%%%%%%%
\pagebreak
\begin{center}
  \textbf{\Large \textsf{Group polarization replications may be marred by high false discovery rates (Supplementary Material)}}
\end{center}

%%%%%%%%%% Prefix a "S" to all equations, figures, tables and reset the counter %%%%%%%%%%
\setcounter{equation}{1}
\setcounter{figure}{0}
\setcounter{section}{0}
\setcounter{table}{0}
\setcounter{page}{1}
\makeatletter
\renewcommand{\theequation}{S\arabic{equation}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\thesection}{S\arabic{section}}
\renewcommand{\thepage}{S\arabic{page}}

\section{Analysis}

\begin{figure}
  \caption{Boxplot of Cohen's $d$ for the ordered probit model over 1000
    simulation trias for each experimental condition (y-axis in A and B). Each
    trial represents a possible outcome of a group polarization experiment where
    the true opinion shift is zero. The mean across trials is closer to 0 than
    in the metric case, but many simulated zero-shift experiments still result in
    an observed shift. This illustrates the weak statistical power of these group
    polarization experiments.}
  \label{fig:OrdinalBoxplot}
  \centering
    \includegraphics[width=1.0\textwidth]{Figures/Analysis/ordinal_cohens.pdf}
\end{figure}


\end{document}
