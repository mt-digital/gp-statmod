\subsection*{Abstract}\label{abstract}
\addcontentsline{toc}{subsection}{Abstract}

\emph{Group polarization} is the name for a form of consensus where
members of a like-minded group become, on average, more extreme in their
opinions after discussing a topic. Group polarization is important
because it may increase social and political tensions if moderates
become more extreme. Decades of studies have replicated detections of
group polarization, typically using an ordinal scale to measure
opinions. However, these studies did not account for ordinal
measurements, which can result in spurious detections of opinion change.
Lacking original data we can still calculate the probability that a
published detection of group polarization was spurious, i.e., to
calculate the \emph{false detection rate}. Across 54 group polarization
experimental conditions from ten representative journal articles, we
calculated false detection rates to be between 0.52 and 0.88, with a
median of 0.75, using a generative model of group polarization
experiments seeded with empirical data. We also use our model to develop
experimental designs that achieve an acceptable false discovery rate.
Much of group polarization research may be unreliable. This analysis can
help change that by enabling others to avoid this hidden pitfall. More
broadly this work demonstrates one important way that replication
success alone does not imply epistemic reliability.

\section{Introduction}\label{introduction}

\begin{quote}
In our introductory social psychology course, we have for many years
used the {[}group polarization experimental paradigm{]} as a laboratory
exercise. The exercise works beautifully, but one must be careful to
forewarn a class that {[}group polarization{]} does not occur with every
group\ldots and that the effect is not large (Brown 1986).
\end{quote}

\begin{quote}
One of the most robust findings in social psychology is that of attitude
polarization following discussion with like-minded others (Cooper,
Kelly, and Weaver 2001).
\end{quote}

If an extremist's opinion falls and it is measured with a Likert scale,
will it make a noise? The answer had better be yes if we want to
properly measure opinion change. Accurate measurements are critical for
developing rigorous and reliable methods to promote more stable,
sustainable, responsive, and efficient governments and institutions
(Mason 2018; Klein 2020). Unfortunately, social and behavioral
scientists have often used statistical methods that distort opinion
change when it's measured on an ordinal, Likert-style scale (e.g.~-3
indicates ``Strongly disagree'', +3 indicates ``Strongly agree'', and 0
is neutral) (Liddell and Kruschke 2018). This includes a phenomenon
called \emph{group polarization} that former Obama White House official
and law professor Cass Sunstein relied on to explain why ``people become
extremists'' and why ``political and cultural polarization'' is ``so
pervasive in America'', as the publisher summarizes (Sunstein 2009).
Sunstein suggests group polarization research provides a ``clue'' to
explaining how ``facism\ldots student radicalism\ldots Islamic
terrorism\ldots{[}t{]}he Rwandan genocide\ldots{[}e{]}thnic
Conflict\ldots acts of torture and humiliation by American soldiers at
Abu Ghraib'', and more (p.~1).

\begin{itemize}
\tightlist
\item
  We demonstrate here that there is a pervasive problem observed over
  decades of group polarization research where faulty statistical
  methods and incomplete reporting that should force the retraction of
  these studies from the canon. Effectively, then, empirical support for
  the theoretical concept \emph{group polarization} stops being so: the
  epistemic integrity of \emph{group polarization} is undermined with
  its empirical foundation losing mass.
\end{itemize}

We complement this critical analysis with a prescriptive analysis to
demonstrate that more appropriate statistical models do indeed lower the
false discovery rate by committing fewer false positive detections of
group polarization under a null model with shrinking opinion variance.
Recently, statistical best-practice is to use Bayesian models of
statistics, where social dynamics and opinion-reporting behaviors are
represented as causally-dependent probability distributions, where a
sample taken from one distribution (e.g., the opinion distribution of a
group in a group polarization experiment) acts as an input parameter to
another distribution (e.g., that represents the
individually-heterogeneous distortion of opinions when reported on an
ordinal scale). Our models follow current best practices in
psychometrics, social science, and Bayesian inference (Liddell and
Kruschke 2018; Kruschke 2015; McElreath 2020; Deffner et al. 2024), but
consolidate them in a new, useful way that enables the retroactive
discovery of null model parameters that generate spurious group
polarization, forcing published evidence for group polarization to be
classified as non-identifiable, and removed from evidence that can be
said to support group polarization.

Our methods can be turned on any data-driven social science analysis of
data where participants report a latent, subjective psychological
variable (mood, physiological state, etc.), and where various theories
supposedly battle it out to best represent the phenomenon of interest
via NHST performed on ordinal observations with weak, informal
theoretical models to motivate the studies. This may just be the first
of many bricks to fall since \emph{all 68} of the articles that
mentioned the word ``Likert'' and analyzed ordinal data used metric
models to detect latent psychological variable change across three
influential psychology journals, \emph{Psychological Science}, the
\emph{Journal of Experimental Psychology: General}, and the
\emph{Journal of Personality and Social Psychology} (Liddell and
Kruschke 2018).

This work is motivated by a core conviction that better thinking leads
to better science. Theoretical clarity, careful measurement, and
principled statistical modeling are not academic luxuries, but essential
tools for empirical science. Theoretical and statistical confusions in
the group polarization literature have lain dormant, undermining
measurements of opinion change in group polarization unbeknownst to
generations of researchers (Brown 1986).

\subsection{Identifiability and the Illusion of
Change}\label{identifiability-and-the-illusion-of-change}

\begin{itemize}
\tightlist
\item
  Many group polarization studies report opinion change based on
  pre/post differences, but assume a transparent mapping between
  observed scores and latent beliefs.
\item
  Yet ordinal opinion data can mask deeper structural ambiguities that
  invalidate these assumptions.
\item
  In what follows, we first describe the basic structure of group
  polarization experiments, then explain how ordinal measurement can
  lead to illusory shifts.
\item
  Finally, we introduce a null-consistency modeling procedure that
  operationalizes this insight, and show how it informs the credibility
  of reported effects.
\end{itemize}

\subsubsection{Experimental structure of group polarization
studies}\label{experimental-structure-of-group-polarization-studies}

\begin{itemize}
\tightlist
\item
  Most group polarization experiments follow a familiar structure:
  measure individual opinions, allow group discussion, then re-measure.
\item
  The central claim is that like-minded individuals become more extreme
  after deliberation.
\item
  Crucially, opinions are typically measured with Likert-style ordinal
  scales (e.g., from ``Strongly disagree'' to ``Strongly agree'').
\item
  Analyses often treat these as interval-scaled, calculating mean
  differences and testing for significance.
\end{itemize}

\subsubsection{The ordinal-continuous mismatch
problem}\label{the-ordinal-continuous-mismatch-problem}

\begin{itemize}
\tightlist
\item
  Ordinal scores do not have a fixed interval structure --- they
  represent ranked categories, not true distances.
\item
  A given change in score may reflect a shift in opinion, increased
  certainty, or both.
\item
  For example, if opinions become more tightly clustered (lower
  variance) post-deliberation, the group mean can appear to shift even
  when the latent mean remains unchanged.
\item
  This opens the door to \emph{null-consistent detections} --- apparent
  effects that emerge from measurement artifacts rather than real
  opinion movement.
\end{itemize}

\subsubsection{Description of the null-consistency modeling
procedure}\label{description-of-the-null-consistency-modeling-procedure}

\begin{itemize}
\tightlist
\item
  We formalize this concern using a generative model that holds the
  latent opinion mean constant while allowing variance to shrink.
\item
  The goal is to test whether an observed group polarization result
  could arise from such a stable-mean, shifting-variance process.
\item
  If this model can reproduce the pre/post measurement pattern, the
  observed result is non-diagnostic --- it is consistent with a null
  hypothesis of no true change.
\item
  This is a form of constructive falsification: it doesn't prove the
  effect is false, but shows it is not uniquely explained by change.
\end{itemize}

\subsubsection{Implications for statistical power and detection
credibility}\label{implications-for-statistical-power-and-detection-credibility}

\begin{itemize}
\tightlist
\item
  Even if an effect is real, detecting it reliably requires statistical
  power --- the ability to distinguish true shifts from noise or
  artifacts.
\item
  Studies using ordinal measures and small samples may be especially
  vulnerable to false positives under the null.
\item
  To assess this, we fit Bayesian ordered probit models to simulated
  data and estimate the power to detect effects of various sizes.
\end{itemize}

\subsubsection{Transition to empirical review and FDR
estimation}\label{transition-to-empirical-review-and-fdr-estimation}

\begin{itemize}
\tightlist
\item
  Having established a model-based definition of null-consistent
  detection, we now apply it to published group polarization
  experiments.
\item
  By combining this logic with simulations seeded from published data,
  we estimate false discovery rates across conditions.
\item
  This isn't a judgment about whether past detections were true. It's a
  demonstration of how to estimate false detection rates
  properly---using models that actually fit the structure of the data.
  We calculate FWER and FDR for the original study designs at three
  effect size levels---small, medium, and large---based on Cohen's
  guidelines (Cohen 1988, Ch. 2.2.3) and Bayesian methods from Liddell
  and Kruschke (Liddell and Kruschke 2018). The verdict is clear: even
  if these studies had used better models, most still wouldn't have had
  the power to rule out false positives. That makes replication a weak
  signal of reliability. If future group polarization studies want to
  produce meaningful result need stronger designs---and that starts with
  a clean break from outdated statistical habits.
\end{itemize}

The null-consistency procedure functions as a test of model
indistinguishability: it demonstrates that a reported group polarization
effect could plausibly arise from a stable-latent process combined with
changing opinion precision. This logic echoes foundational concerns in
causal inference, where two competing models---one causal, one not---can
imply the same observed data distribution and are therefore empirically
indistinguishable without further assumptions (Pearl 2000; Spirtes,
Glymour, and Scheines 2000). In such cases, inference from observation
to mechanism is invalid. Similarly, in psychometrics, it is well known
that different configurations of latent traits and response thresholds
can yield identical observed ordinal patterns, making the true structure
unidentifiable without additional constraints (Borsboom 2005). The
present approach does not attempt to estimate the probability of false
detection, but rather shows that under ordinal measurement, a null model
can reproduce the observed pattern---rendering the result non-diagnostic
of a true latent shift. This establishes a stringent standard: an effect
must not only be detected, but shown to be incompatible with plausible
null-generating mechanisms.

