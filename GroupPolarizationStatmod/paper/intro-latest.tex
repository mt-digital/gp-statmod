\subsection*{Abstract}\label{abstract}
\addcontentsline{toc}{subsection}{Abstract}

\emph{Group polarization} is the name for a form of consensus where
members of a like-minded group become, on average, more extreme in their
opinions after discussing a topic. Group polarization is important
because it may increase social and political tensions if moderates
become more extreme. Decades of studies have replicated detections of
group polarization, typically using an ordinal scale to measure
opinions. However, these studies did not account for ordinal
measurements, which can result in spurious detections of opinion change.
Lacking original data we can still calculate the probability that a
published detection of group polarization was spurious, i.e., to
calculate the \emph{false detection rate}. Across 54 group polarization
experimental conditions from ten representative journal articles, we
calculated false detection rates to be between 0.52 and 0.88, with a
median of 0.75, using a generative model of group polarization
experiments seeded with empirical data. We also use our model to develop
experimental designs that achieve an acceptable false discovery rate.
Much of group polarization research may be unreliable. This analysis can
help change that by enabling others to avoid this hidden pitfall. More
broadly this work demonstrates one important way that replication
success alone does not imply epistemic reliability.

\section{Introduction}\label{introduction}

\begin{quote}
In our introductory social psychology course, we have for many years
used the {[}group polarization experimental paradigm{]} as a laboratory
exercise. The exercise works beautifully, but one must be careful to
forewarn a class that {[}group polarization{]} does not occur with every
group\ldots and that the effect is not large (Brown 1986).
\end{quote}

\begin{quote}
One of the most robust findings in social psychology is that of attitude
polarization following discussion with like-minded others (Cooper,
Kelly, and Weaver 2001).
\end{quote}

If an extremist's opinion falls and it is measured with a Likert scale,
will it make a noise? The answer had better be yes if we want to
properly measure opinion change. Accurate measurements are critical for
developing rigorous and reliable methods to promote more stable,
sustainable, responsive, and efficient governments and institutions
(Mason 2018; Klein 2020). Unfortunately, social and behavioral
scientists have often used statistical methods that distort opinion
change when it's measured on an ordinal, Likert-style scale (e.g.~-3
indicates ``Strongly disagree'', +3 indicates ``Strongly agree'', and 0
is neutral) (Liddell and Kruschke 2018). This includes a phenomenon
called \emph{group polarization} that former Obama White House official
and law professor Cass Sunstein relied on to explain why ``people become
extremists'' and why ``political and cultural polarization'' is ``so
pervasive in America'', as the publisher summarizes (Sunstein 2009).
Sunstein suggests group polarization research provides a ``clue'' to
explaining how ``facism\ldots student radicalism\ldots Islamic
terrorism\ldots{[}t{]}he Rwandan genocide\ldots{[}e{]}thnic
Conflict\ldots acts of torture and humiliation by American soldiers at
Abu Ghraib'', and more (p.~1).

% \begin{itemize}
% \tightlist
% \item
We developed a generative model of opinion change to identify, if possible,
null opinion change models that could spuriously appear like group polarization
when naively measured using a Likert scale without accounting for the measurement procedure. 
We further applied our method to estimate the family-wise error rate of
detections of group polarization in a corpus of ten influential papers from 
across several decades and from various theoretical traditions. We turn this criticism of past
blunders constructive by providing a procedure to calculate what Cohen's $d$ should be
used to guarantee a family-wise error rate (i.e., Type I error rate) of 5\% or less.

We calculate the severity of the problem in two steps. In the first step we 
provide proof by example that there exists a null opinion change model in terms of \emph{latent, continuous}
psychological opinions that looks like group polarization when measured via participant behavior, i.e.,
by their response on an \emph{ordinal survey scale}. If such a null model is found, this means that we cannot
rule out a null effect, given the published data. This means, therefore, that the published detection is
just as likely false as it is true, so we say it is a \emph{plausibly false detection of group polarization},
or a \emph{plausibly spurious detection} for short. 

In the second step of the analysis of whether, we simulate observations
of group polarization experiments where participant opinions are drawn from the 
null model distributions identified in the first step,
then these are measured on a Likert scale for simulated group polarization survey
responses on an ordinal scale. We then calculate the
family-wise error rate (i.e., Type I error rate) and false detection rate using
various effect sizes to determine significance of the effect in terms of Cohen's $d$.
The family-wise error rate is defined as the probability that a known null 
effect is detected as significant. We use this to then estimate the *false detection rate*, which is the
probability that a detection of an effect is a false one, which depends on the relative prevalence of
true and false positive detections in the literature.
Finally, we put our model to constructive use to show what Cohen's $d$ must be set to
to limit the family-wise error rate to 5\%, denoted $d*$. This will not help rescue any of
the published results here found to be plausibly spurious. But, it provides a measure of the weakness of
the published experimental designs independent of their shared fatal failure to 
account for the measurement procedure: greater $d*$ indicates weaker designs.

We find that 95\% (54 of 57) of group polarization detections are plausibly spurious.
In other words, empirical support for group polarization has been seriously undermined
just by removing these results from the count of group polarization replications.
There is no reason to expect other papers to be more reliable. (Summarize other
two findings, 1. FWER/FDR and 2. $d*$ for FWER < 5\%)

Our methods can be turned on any data-driven social science analysis of
data where participants report a latent, subjective psychological
variable (mood, physiological state, etc.), and where various theories
supposedly battle it out to best represent the phenomenon of interest
via null-hypothesis significance testing using ordinal data as continuous. 
This may just be the first
of many bricks to fall since \emph{all 68} of the articles that
mentioned the word ``Likert'' and analyzed ordinal data used metric
models to detect latent psychological variable change across three
influential psychology journals, \emph{Psychological Science}, the
\emph{Journal of Experimental Psychology: General}, and the
\emph{Journal of Personality and Social Psychology} (Liddell and
Kruschke 2018). This further confirms that replication is far from the only
foundational crisis that behavioral science faces\cite{Yarkoni2022,Turner2022}. 

Replication is not enough:
strong, mechanistic models must be developed to be able to predict Type I and II
error rates \emph{ahead of time}, as we demonstrate here. We only demonstrate how to
calculate expected Type I error rates for a measurement design, i.e., the rate of false 
positives. Low statistical power exacerbates false detection rates. So, to be confident in replication,
a pre-registered measurement design should use this style of simulation to 
estimate the statistical power. Tangled, weak, informal
verbal models of complex psychological and social phenomena like those that motivated
empirical studies of group polarization tend to be vulnerable
to stastical weaknesses. It is difficult to incorporate prior knowledge such
as experiment structure without a sufficiently rigorous and appropriate 
formal system \cite{Smaldino2017,Smaldino2020,Flake2020,Deffner2024}.

\subsection{Group polarization research}

(DO THE MINIMUM AFTER EVERYTHING ELSE)

To situate this current paper it will help to introduce some relevant research
traditions and common experimental designs in group polarization, using the ten
studies included in our corpus as touchstones. To begin, *group polarization*
is defined as...(continue basic definition with some examples from our corpus).

Paragraph dos: 
Common experimental, measurement, and statistical procedures to induce and
detect group polarization

\subsection{How study design can induce false detections of group polarization}
\label{ssubsec:false-detections}


An opinion is never measured directly, only a behavior that indicates an opinion
in a behavioral experiment or survey.  For example, the location of a mark
that a participant makes is measured, not the complex neural activity that
underlies forming opinions.
Theoretically, an internal opinion in the form of neural activity is a
\textit{latent opinion}, from the Latin \emph{latentem} meaning ``lie
hidden''.  Participants report \text{observed opinions}, typically as one of
a finite number of ranked, successive integer values that form the
\emph{selection set}. The minimum possible value in the selection set might
mean ``strongly disagree'' and the maximum value represents ``strongly
agree''.

Group polarization experiments share a common experimental design where,
first, participant opinions are gathered on a topic of interest. Next, groups
are formed that have some initial opinion extremism. Participants deliberate
or discuss the topic for some set time. After the set time, participants
again give their opinion. Group polarization is said to occur if the
mean group opinion became more extreme, i.e., if the magnitude of the mean
group opinion is greater \emph{post-deliberation} compared to
\emph{pre-deliberation}. For example,
\cite{Moscovici1969} asked Parisian high school students to indicate their
degree of agreement with the statement, ``American economic aid is always
used for political pressure''. There
were seven options in the selection set, -3 indicating ``strongly disagree'',
+3 indicating ``strongly agree'', and 0 was neutral. Moscovici and Zavalloni
report that the mean pre-deliberation opinion was -0.61 and the mean
post-deliberation opinion was -1.09 (Moscovici and Zavalloni, 1969; Table 4,
p. 132). This is a -0.48 shift to more extreme disagreement with the 
statement, reported by Moscovici and Zavalloni as statistically significant
in Table 5 on p. 132.

(PREVIOUS TWO PARAGRAPHS SET UP THIS FINAL PARAGRAPH: CONTINUE TO USE MOSCOVICI 1969 TO
EXPLAIN IN THE SIMPLEST TERMS HOW FAILING TO ACCOUNT FOR MEASUREMENT PROCEDURE
CAN LEAD TO FALSE POSITIVES; see Figure \ref{fig:distros})


\subsection{Overview}

In the next section, we introduce the various formal and computational Methods 
used to obtain our findings outlined above. We then explain our findings in detail in the
Analysis section. We close with a Discussion of how this is not a defeat, but
a new beginning for group polariation, given that there are good reasons to expect
group polarization to occur. We also explain how to expanding this analysis to
examine other published studies on group polarization and across social and behavioral
science disciplines.
