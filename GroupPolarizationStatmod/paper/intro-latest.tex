\subsection*{Abstract}\label{abstract}
\addcontentsline{toc}{subsection}{Abstract}

Echo chambers seem to cause radicalization—and radicalization is risky.
Radicalization undermines institutions by reducing their diversity, 
which in turn impairs their adaptability.  Social psychologists and others study
echo chambers experimentally using \emph{group polarization} experiments.
Researchers induce \emph{group polarization} by first asking experiment
participants their opinions on some topic, then grouping them together if
they agree in spirit, but not degree. The participants then discuss the topic
in their groups. Their opinions are then re-measured. Group polarization is
when the average group opinion increases. But there’s a fatal problem:
participants report opinions on ordinal scales, but this has never been
accounted for statistically. This can cause apparent group polarization in
observations even if the average latent opinion—the one in people’s
heads—doesn’t change, due to ceiling effects. Here we prove that across ten
journal articles spanning five decades that 54 of 57 reported polarization effects are plausibly false due to this artifact, and thus cannot be counted as replications. Our method can be used to evaluate any experimental design that measures latent change with ordinal scales. Given how destructive radicalization can be, we must get this right.


\section{Introduction}\label{introduction}

If an extremist moderates their opinion and it is measured with a Likert scale, will
this move to moderation be detected? The answer is often no in social psychology 
experiments~\cite{Liddell2018}. This
means that a group can appear to become radicalized over time even if the 
average opinion does not actually change~\cite{Turner2020}. 
This has huge consequences for the study of \emph{group polarization}, the hypothesis
that discussion among like-minded people pushes opinions toward extremes—effectively 
making it the science of ``echo chambers'', online or in-person communities 
composed of likeminded folks.  Cass Sunstein, a
Harvard Law professor and former White House administrator, has written
extensively on the topic~\cite{Sunstein2019}, claiming that group
polarization is a scientific law—a regular, quantitative relationship that
will always be observed when certain assumptions are
met~\cite{Cartwright1999}. Yet Roger Brown himself noted in Social Psychology
that group polarization ``does not occur with every group'' and that ``the
effect is not large''~\cite{Brown1986}. 
Given the influence of this theory and the hundreds of claimed ``replications'' of
group polarization, it is essential to ask whether any of these replications may
instead be \emph{measurement artifacts}—spurious effects created when the
``neutral'' point of the measurement scale fails to align with the group’s own
center of opinion. The science of opinion change has a central role in designing new
methods for social organization and social change to face pressing existential
challenges~\cite{Galesic2021,Galesic2023}.

The group polarization hypothesis is that discussion within a group of
likeminded people causes the group's average opinion to increase, or
\emph{polarize}~\cite{Brown1986,Brown2000}.
Group polarization experiments therefore attempt to
polarize groups as follows: first, participants report their opinion on
some topic or prompt chosen by the experimenters in advance. Second, 
participants are put into likeminded groups to
discuss the topic. Following discussion, participants again report their
opinions on the topic. Group polarization is defined as an increase in
the average opinion following discussion. If the group polarized, and we 
assume that people in the group found consensus—meaning their opinions agreed
more after the discussion than before—then initially moderate opinions must have
increased more than more extreme ones.

We found that 95\% of experimental reports of group polarization
across sixty published experimental conditions cannot be counted as
replications. We explain here how evidence of group polarization is undermined
by a basic measurement flaw that obscures opinion change among extremists.
This opens the possibility that what seems to be group polarization could just
be \emph{mere agreement}, with no change in mean opinion, if only increases in
opinion were measured but not decreases.  To evaluate whether the empirical
record reflects genuine opinion change or merely measurement artifacts, I
simulated published experiments under conditions of mere agreement. Each
simulation reproduced the core elements of published designs—pre-discussion
survey, group interaction, post-discussion survey—while holding the latent
mean opinion constant. Because the original datasets are lost and published
results rarely provide enough information to reconstruct them, we cannot know
whether any reported effect reflects a real shift or a measurement artifact.
When distinct latent processes can generate indistinguishable results, the
effect is non-identifiable—and no claim of replication can stand.

A relatively simple step could fix the problem, we believe: measure twice
before discussion. Before forming groups, experimenters could poll opinions
using a scale centered arbitrarily at 0, with varying strengths of disagree
or agree with negative and positive integers, respectively. 
Then after likeminded groups are
selected, measure group opinions again before they discuss, but now with the
scale centered on the average group opinion. To keep the same number of bins,
add more bins to express more extreme views and take away bins from
the opposite side as necessary.

Even if this measurement problem were fixed, equally severe problems remain
for group polarization research. First there seems to be no stable 
``opinions'' per
se—it is more accurate to say people construct opinions when they are
prompted or need them for some other reason, as revealed by respondents
answering questions differently if the framing or ordering of them changes,
and by the fact that people respond differently to the same question over
time. These are just a few of several potential sources of variance that were never accounted for, which is also known to generate false detections of change. 
Finally, because group polarization experimental design was never
standardized its research corpus cannot cumulatively or systematically tell 
us anything general about the world—Dorian
Cartrwright (1973) tried to alert group polarization researchers to this
problem half a century ago~\cite{Cartwright1973}. General scientific claims
of the existence and functioning of some scientifically induced phenomenon
only work if the evidentiary observations were collected from
\emph{commensurate} experiments, or in other words, only from experiments
that share the exact same causal structure as expressed in a formal or 
mechanistic causal model~\cite{Cartwright1999}.



\subsection{Group polarization research}

Group polarization research began in the 1960s, which grew rapidly with
focused government funding coupled with widespread
interest in what was then a surprising new science of
radicalization~\cite{Cartwright1971}. Dorian Cartwright (1973) reviewed the
first decade of group polarization research and found problems: he lamented
the ``disturbing fact'' that group polarization research has produced no
cumulative theoretical undersatnding of the effects of discussion on
group opinions~\cite{Cartwright1973}. Cartwright was somewhat dismayed, but
optimistically hoped that group polarizaiton research would eventually
produce ``an enduring\ldots more comprehensive paradigm'' \cite[p.
230]{Cartwright1973}.

Cartwright (1973) suggested that a ``vigorous search for alternative
formulations'' could help group polarization become more useful. More
researchers developed specific theories of group
polarization, with two becoming particularly influential: social comparisons
and persuasive arguments. The social comparisons hypothesis says group
polarization occurs because humans want to fit in, and so generally like to . Group polarization occurs, then, when a
group is composed mostly of extremists, with a sufficient number of people
with a weak opinion \cite{Myers1970; Myers1975}. Its main
“competitor” from the 1970s was the persuasive arguments theory, which
basically claimed that rhetoric was the strongest driver of group
polarization, not social comparisons \cite{Burnstein1973,Burnstein1975}.

Other notable hypotheses include the
self-categorization and social decisions scheme hypotheses.
Self-categorization augmented the social comparisons hypothesis with a
consideration of idiosyncratic beliefs people hold that are the result of
their own personal lived experience \cite{Abrams1990,Hogg1990}.
The social decisions hypothesis posits that contextual factors like
social network structure could be at least as important as the distribution
of opinions and . Noah Friedkin, a sociologist, notably published
the study with the least potential measurement artifacts for a rigorous study on group polarization, adopting the social
decisions scheme hypothesis, predicting that social connectivity within a
group could amplify group polarization \cite{Friedkin1999}.


\subsection{Study Corpus} 

We selected ten representative journal articles with sixty total experimental
conditions among them. For a representative sample of group polarization
research across decades, we two articles for three of the five article
categories: \emph{Social Comparisons}, \emph{Persuasive Arguments},
 and \emph{Politics}. We included three \emph{Self-Categorization} studies and one \emph{Social Decisions}.
All but the \emph{Politics} category refers to the theoretical tradition that the
authors were trying to prove right—the two \emph{Politics} articles were agnostic
to group psychology theories. This resulted in a representative sample research
over time, as well: we have one study from 1969,
four from the 1970s, three from the 1990s, one from 2007 and one from 2010. 
Information about our corpus is summarized below with references 
(Table~\ref{tab:articles}).  

\include{article-table.tex}

Each study used subtly different experimental methods that make it difficult
to build cumulative general knowledge. But all of the studies share one thing
in common: they all measure participant opinions on an ordinal scale. We start
with a non-technical review of how ordinal opinion measurements work in group
polarization, and then how the distributions of measurements are analyzed to
determine if group opinions became polarized. Then we can explain our
close-reading analysis to extract the necessary experimental design parameters
we need to simulate all the experiments in our corpus. 

The corpus contains
sixty different experimental conditions in the ten corpus journal articles. We 
simulate experiments for all 57 experimental conditions reported to have 
replicated group polarization, out of 60 total conditions (three were
null control conditions). 

Using simulations we will estimate the number of these 57 replications that
must be retracted because null-polarization simulations appear polarized due
to measurement artifacts. We collect experiment design parameters that fill out 
details of the simulation model of group polarization experiments. 

\subsection{Ordinal Opinion Measurements: Problems and Simulations}

In studies of group polarization, opinion measurements are typically ordinal,
reflecting ordered positions rather than measurable distances between them—
Moscovici and Zavalloni (1969) gave participants a seven-point scale, from
“strongly disagree” (-3) to “strongly agree” (+3) to report their opinions.
Burnstein and Vinokur (1975) used a ten-point scale from 1 to 10 representing
“the lowest odds of success acceptable” to try a certain behavior, where 1
means 10\%.  In these and every other survey-based group polarization study I
know, the reported opinions on the ordinal scale—with discrete numbered
bins—were treated as if they were continuous-valued, or in other words as if
they were the internal mental opinion value itself, whatever that might be
neurobiologically. 

Note that ordinal scales clip all latent opinions with a magnitude greater
than the magnitude of its extreme values. This can make null-polarization
outcomes look like group polarization. Consider the following example 
with a group of just two people to see why: say they start with latent
opinions of 2.1 and 4.7, which they report as +2 and +3, let's say, on Moscovici
and Zavalloni's scale that tops out at +3. If they then discuss the topic and their
opinions move toward one another by 1 each, then their latent opinions would be 3.1
and 3.7. Before and after discussion the latent mean was a constant 3.4. 
When reported on an ordinal scale, both opinions are now +3—the average measured
opinion \emph{spuriously increased} from 2.75 to 3.0!

To determine whether a replication must be retracted, we developed a model to
simulate measurements of null-polarizations, labelled \emph{mere agreement}, where
the mean group opinion stays constant, but variance decreases. In other words,
simulated participants report more agreement following discussion, 
but they do not become radicalized.  

The simulations are seeded by actual group polarization experiment parameters.
We copied the pre- and post-discussion opinion means reported in each article.
There is no original data available for any of the experiments. Sometimes more
information was available about the pre- and post-discussion
distributions—we used this information to decide whether replications plausibly 
have null-polarization. We recorded all available data with a guide to finding it in
its original article in the file `StudiesAnalysis.csv`, available with the
Supplemental Information or to view/download on
GitHub\footnote{\url{https://github.com/mt-digital/gp-statmod/GroupPolarizationStatmod/data/StudiesAnalysis.csv}}.


\subsection{Research Overview}

We collected original data across 10 journal articles to show that 95\% of 
the replications reported must be retracted due to non-identifiability with a
null-polarization model. These replications cannot be salvaged because the
original data is no longer available, and thus cannot be re-analyzed with a 
better statistical model that properly accounts for the measurement procedure. 

In the Analysis we go further: we use simulations to show that 
the experimental designs in our corpus are beset by high false positive rates 
\emph{even with a better statistical model}. The only way to reduce false
positive rates with the experimental designs used in the papers is to increase
the significance threshold (in terms of Cohen's $d$ as explained in more
detail in the Methods).

Group polarization was taken as one of the most reliable group effects in social
science—but we showed it is impossible to distinguish 95\% of group
polarization detections from a null-polarization model. This means 54
replications must be retracted—and recall that this was a selection of the
highest quality, most representative group polarization research. 
To understand group polarization, new
methods will be required. 

This sort of error threatens our ability to construct a
cumulative science of opinion change, social influence, and social change
generally. We need reliable social science as the foundation for new
strategies for social organization capable of reversing decline and powering
sustainable futures. We have a long way to go.
