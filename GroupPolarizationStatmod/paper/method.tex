\section{Method}\label{method}

Our method combines several formal and computational tools to identify which
replications should be retracted and evaluate experimental designs across our
corpus. The first step in the analysis is to find a reasonable pair of latent
null-polarization distributions that appear polarized due to clipping
(Section~\ref{subsec:identify}). To do this, we develop a formal model and a numeric
rootfinding algorithm to power a Shiny web app we used to interactively identify and
record null-polarization parameters generating such measurement artifacts. 

The second step in the analysis is to evaluate the quality of the experimental designs
when we properly account for the ordinal measurement procedure
(Section~\ref{subsec:evaluate}). We use the null-polarization parameters we found in
the first step to generate simulated experimental data, then estimate the difference
between the pre- and post-discussion means using ordered probit models to fit each
distribution—ordered probit models account for the ordinal measurement procedure.  We
estimate the false positive rate and false detection rate for low, medium, and high
significance thresholds in terms of Cohen's $d$. Finally, we use the effect sizes thus
calculated to determine the significance threshold one must use to achieve a 5\% false
positive rate.


\subsection{Identify replications for removal}
\label{subsec:identify}

The first step is to find which replications of the 57 must be retracted due to
non-identifiability. We establish non-identifiability by finding one set of latent
null-polarization opinion distributions where the mean doesn't change, but appear
polarized when measured on the ordinal scale used in the experiment.  In other words,
if we find a null-polarization model that explains the outcome as well as a group
polarization one, the group polarization hypothesis is \emph{non-identifiable} and the
replication must be rejected—the data cannot confirm or disconfirm the target
hypothesis.

To do this we designed a web app that allows the user to try different latent
distribution parameters (Figure~\ref{fig:data-model-flow}) to seed a
hillclimbing algorithm that explores latent distribution parameter space
until it finds null parameters that look like group polarization under the
experiment's ordinal measurement procedure, or the algorithm stops after a
certain number of steps (Algorithm~\ref{alg:hillclimbing}). If the web app
finds a constant latent mean and pre- and post-discussion 
variances to specify a pre- and post-discussion opinion distributions that
generate spurious group polarization, this is communicated to the user who
can then mark the experimental condition as such.

To find examples of plausibly spurious group polarization we guess at what latent
mean, combined with a latent pre- and post-deliberation latent standard
deviations, could generate it. We calculate the observed group polarization
for these latent parameters by transforming the latent pre- and
post-deliberation distributions from continuous normal distributions to
ordinal ``observations'' by integrating over each ordinal bin, then
calculating the observed mean for each experiment \emph{phase}, i.e., both
the pre- and post-deliberation phases.  If the null model can generate group
polarization in this case, then the detection is labelled \emph{plausibly
false}, i.e., \emph{plausibly spurious}, and we call the generating latent
parameters the \emph{plausible null parameters}. 

To evaluate whether the plausible null parameters we identify could
realistically fool a significance test, we implemented a generative
simulation of each plausibly false detection of group polarization and
calculated the \emph{family-wise error rate (FWER)} and the \emph{false
discovery rate (FDR)}. We use the published number of participants $N$ for
each of the 54 conditions that yielded plausibly false detections of group
polarization. The FWER is also known as the Type I error, or the false
positive rate: it is the probability that a detection of an effect occurs
given that in reality there was a null effect.  The FDR is the probability
that a published positive detection of an effect is a false positive, i.e., a
detection of a null effect. High FWER and FDR for an experimental condition
indicates that its design is weak, since we identify only one of perhaps
infinitely many latent parameter combinations that can generate spurious
group polarization. 

We use this setup to demonstrate how \emph{adversarial} style testing of
experimental designs with counterfactual null models can be used to estimate
important metascience parameters. In this study we use our FWER calculations
to estimate the minimum Cohen's $d$ that should be used to determine whether
an effect is ``significant'' or not that will limit the FWER to 5\%. In the
Discussion we sketch how our model can then be extended to rigorously
calculate statistical power, i.e., the probability that a significant effect
is detected given that there is indeed a significant effect.

\begin{table}[h]
  \caption{\textbf{Latent psychological variables and distributions.}}
  \begin{tabular}{cll} 
  \toprule
    \textbf{Variable} & \textbf{Description} & \textbf{Values} \\
  \midrule  
    $\omega_{i,t}$ & Latent opinion of individual $i$ at time $t$ & $\mathbb{R}$ \\
    $\mu_t$ & Mean latent opinion at time $t$ (constant in null-polarization) & $\mathbb{R}$ \\
    $\sigma_t$ & Variance of observed opnions at time $t$ & $\mathbb{R}$ \\
  \bottomrule
  \end{tabular} 
\end{table}

\begin{table}[h]
  \caption{\textbf{Experimental design and numerical model parameters.}}
  \begin{tabular}{cll} 
  \toprule
    \textbf{Variable} & \textbf{Description} & \textbf{Values} \\
  \midrule  
    $B$ & Number of bins & $5,6,7,\ldots$ \\
    $\theta_b$ & The $B+1$ threshold values that separate bins & 
      e.g., $-\infty, -2.5, -1.5,\ldots,2.5, \infty$ \\
    $b$ & Bin index—there are $B+1$ to mark $B$ bins & $0,1,2,\ldots,B$ \\
  \bottomrule
  \end{tabular} 
\end{table}

\begin{table}[h]
  \caption{\textbf{Simulated observed variables.}}
  \begin{tabular}{cll} 
  \toprule
    \textbf{Variable} & \textbf{Description} & \textbf{Values} \\
  \midrule  
    $\meanobst$ & Mean of all reported opinions (Eq. \ref{eq:meanobs}) & $\mathbb{R}$ \\
    $g$ & Observed group opinion polarization (Eq. \ref{eq:group-polarization})
        & $\mathbb{R}$ \\
    $d$ & Cohen's $d$ used as significance threshold 
        & $\mathbb{R}$ \\
    $\alpha_e,\alpha_s,\alpha_{\text{all}}$ & Family-wise/Type I error rate
    for an experiment, study, or across all experiments
             & $[0, 1]$ \\
    FDR & False discovery rate
        & $[0, 1]$ \\
  \bottomrule
  \end{tabular} 
\end{table}


\subsection{Probability theory of spurious group polarization}

% We formalize this experimental design in the $N\to\infty$ case where the
% distribution of ordinal measurements is created by integrating continuous latent
% opinion distributions.  


Latent opinions are assumed to be normally distributed in general since they are
aggregations of many neurobiological potentials of
different polarities and magnitudes. We have two opinion distributions for 
two different times in the group polarization experiment process: pre-discussion,
$\tpre$, and post-discussion, $\tpost$.
The 
mean $\mu_t$ and variance $\sigma_t$ for time $t$. We are concerned with only two
times, \emph{pre-} and \emph{post-} deliberation ($t=0$ and $t=T$, respectively).
For short we can write $\mu_\mathrm{pre}$ and $\sigma_\mathrm{pre}$ for
pre-deliberation mean and variance (and similar for \emph{post}-deliberation).  
The latent opinion of participant $i$ at time $t$ is drawn from a normal 
distribution in the model, 
\begin{equation} 
  \omega_{i,t} \sim p(\omega; \mu_t, \sigma_t) = \mathcal{N}(\mu_t, \sigma_t).
  \label{eq:opinionDistribution} 
\end{equation} 
\noindent 

The measurement process with $N\to\infty$ participants is simulated by 
integrating the latent opinion distribution between ordinal bin thresholds, 
of which there are $B+1$, where $B$ is the number of opinion bins, with sequential
integer values $b\{0, B\}$. In the Moscovici and Zavalloni (1969)
example, the thresholds are $\theta_b \in \{-\infty,-2.5,-1.5,\ldots,2.5,\infty\}$, with $B = 7$.
Bin $b$ is defined by its lower and upper continuous-valued thresholds, 
% $\theta_{b-1}$ and
$\theta_{b}$, respectively. Thresholds are given by the following equations
(visualized as dotted lines in Figure~\ref{fig:distros}).
\begin{equation}
\theta_b = \begin{cases}
  -\infty         & \text{ if } b=0 \\
  \infty          & \text{ if } b=B \\
  \theta_{\text{Base}} + \frac{b}{2} & \text{ otherwise.}
\end{cases}
\end{equation}
\noindent
In the Moscovici and Zavalloni example, the base bin value is
$\theta_{\text{Base}} = -3$.
With thresholds defined, the frequency of ordinal measurment $b$ at time $t$ 
is (Figure~\ref{fig:distros})
\begin{equation}
  p(b;~\mu_t, \sigma_t, B) = \int_{\theta_{b-1}}^{\theta_b} p(\omega; \mu_t, \sigma_t)
  d\omega.
  \label{eq:ordinal-frequency}
\end{equation}

We can now define spurious group polarization and give examples of when it
occurs. First, the expected value of observed opinions at time $t$ is
\begin{equation}
  \meanobst = \frac{1}{B} \sum_{b=1}^B b \cdot p(b; \mu_t, \sigma_t, B).
  \label{eq:meanobs}
\end{equation}
\noindent
The observed group polarization opinion shift is then
\begin{equation}
  g = \meanobs_\mathrm{post} - \meanobs_\mathrm{pre}.
  \label{eq:group-polarization}
\end{equation}
\noindent

Measurement artifacts can make null-polarization spuriously appear like group
polarization. To see how, we will build up a formal model of the
process. Consider a collection of
null-polarization latent opinion distribution parameters $\nu = (\mu, \sigma_{pre}, \sigma_{post})$.
The group polarization opinion shift can then be written as a function of
$\nu$,
$g(\nu)$. 

If we can find ..... $\nu$ for which $g(\nu) = g_{pub} > 0$, where
$g_{pub}$ is the published group polarization shift value.
This could be thought of as sampling the observed ordinal distributions 
$N \to \infty$, collecting ordinal measurements of samples from 
each latent distribution corresponding to each phase, pre- and post-deliberation.

Next, we explain how to find a set of latent null parameters, $\nu$, that
generate spurious group polarization when naively measured on an ordinal scale.

\begin{figure}[ht]
  \caption{\textbf{Opinion measurement model and spurious group polarization.} 
  Latent model participant opinions are drawn from a normal distribution with the
  listed parameters (A,C). The pre-deliberation latent distribution has high
  variance, but appears polarized when measured via integration (A).  The latent
  distribution's variance is expected to decrease during deliberation (B) via
  consensus, but in this example the latent mean is static, 
$\mupre = \mupost$. However, when the post-deliberation distribution is measured,
the simulated observed mean increases, i.e., spurious
group polarization occurs (C).}
  \centering
  \includegraphics[width=0.95\textwidth]{Figures/Model/latent-ordinal-distros.pdf}
  \label{fig:distros}
\end{figure}



\subsubsection{Interactive algorithm to find parameters inducing spurious group polarization} 

To identify plausibly false detections of group polarization, we searched numerically
for latent parameters $\nu = (\mu, \sigmapre, and \sigmapost)$ that generate
spurious group polarization, as explained immediately above. We developed an *ad hoc*
hillclimbing-style algorithm that finds $\nu$ that finds $\nu$ by minimizing
the difference between published group polarization shifts $g_pub$ and calculated shifts,
$g_\nu$.
calculated with Equation~\ref{eq:meanobs}. 

The algorithm to find $\nu$ holds $\mu$ constant while searching for $\sigmapre$
and $\sigmapost$ that generate observed $\meanobspre$ and $\meanobspost$, within a
tolerance of $\pm10^{-3}$ (Algorithm~\ref{alg:hillclimbing}). 
The algorithm requires a researcher to specify an initial
guess for $\nu$ and auxiliary parameter settings, which generally vary between
experimental conditions.  Furthermore, the algorithm does not always produce
reasonable distributions with a given $\nu$, and sometimes it does not produce
spurious group polarization.  Therefore, it is necessary to sanity check the latent
and ordinal opinion distributions for the $\nu$ found by the algorithm. If all is
well, the experimental condition can be marked as plausibly spurious.  To tame the
complexity involved in data entry and model fitting for each of these sixty
experimental conditions, we developed a web application to input data from control
the algorithm and inspect its results (Figure~\ref{fig:webapp}). Web application code is
available online in our GitHub
repository\footnote{\url{https://github.com/mt-digital/gp-statmod/blob/main/GroupPolarizationStatmod/app.R}}.

\begin{algorithm}
  \caption{Routine to find latent mean and variance that generate spurious group polarization.}
  \label{alg:hillclimbing}
  \begin{algorithmic}

    \Require $\mu$, $\sigma$, $\bar{\mathbf{o}}$ 
      \Comment{Set latent mean, guess variance that yields, input empirical mean.}

    \Require $\epsilon$, $\delta$, $i_\mathrm{max}$ 
      \Comment{Hillclimbing tolerance, step size, and maximum iterations.}

    \State $\Delta \gets \meanobs(\mu, \sigma) - \meanobsemp$ 
      \Comment{Initialize simulation error.}
    \While {$\Delta^2 > \epsilon$ and $i < i_\mathrm{max}$} \Comment{Search until desired
    accuracy or max iterations reached.}
      \State $\sigma' \sim \mathcal{N}(\sigma, \delta)$ \Comment{Draw new variance to test.}
      \State $\Delta' \gets \bar{o}(\mu, \sigma') - \bar{\mathbf{o}}$ 
        \Comment{Calculate error using new variance.}
      \If {$\Delta' - \Delta < 0$}
        \State $\sigma \gets \sigma'$
          \Comment{Update $\sigma$ if the error is less than before.}
        \State $\Delta \gets \bar{o}(\mu, \sigma) - \bar{\mathbf{o}}$
          \Comment{Update simulation error if $\sigma$ is updated.}
      \EndIf
    \EndWhile \\

    \Return $\sigma$
  \end{algorithmic}
\end{algorithm}

Note that so far, we have only identified cases where spurious group
polarization plausibly occurs in the asymptotic case where $N\to\infty$ ($N$
being the number of participants).  Experimental conditions yielding
plausibly spurious group polarization warrant some suspicion. If there exists
set of latent simple consensus parameters $\nu$ that create binned
(Equation~\ref{eq:ordinal-frequency}) opinion distributions that naively seem
to be group polarized, then the means of at least some samples from these
distibutions with empirical numbers of participants would also appear to be
shifted consistent with group polarization. This process itself does not tell
us the false detection rate.  We use the identified simple consensus
parameters to define pre- and post-deliberation latent opinion distributions
in simulated trials of group polarization experiments.

\begin{figure}
  \centering
    % \includegraphics[width=0.8\textwidth]{Figures/Model/data-model-flow.png}
  \includegraphics[width=\textwidth]{Figures/Model/SoftwareMap_Mashup.png}
  \caption{\textbf{Simulation and analysis pipeline}
  for finding when measurement artifacts.  
  Arrows show the flow of input and output data from parameter estimation.
  First, we used the web app (A) to find null-polarization parameters for 
  each experimental condition, $\nu_e$,
  that ordinal measurements distort to look like polarization
  (B—described in Algorithm~\ref{alg:hillclimbing}).
  We fit ordered probit models to simulated null-polarization outcomes,
  seeded by the $\nu$ found in the previous step (C). Analysis and 
  visualization routines finish the pipeline (D).}
  \label{fig:data-model-flow}
\end{figure}



We calculate via simulation and data fitting
the Type I error rate for the experimental designs used in experimental conditions
that, under a preliminary large-$N$ model analysis, generated group
polarization upon ordinal measurement of simple consensus latent opinion 
distributions. These can then be aggregated to estimate the false discovery rate
for each journal article and for the ten-article corpus, which is representative of the
false discovery rate for group polarization studies. 

The false detection rate is the scaled probability that a detection, $D$, of
an effect, $E$, is in fact a false detection and there is \emph{not} an effect,
denoted $\neg E$. The probability that a detection $D$ has occurred when there
was no effect, i.e. when $\neg E$, is denoted $\alpha = \Pr(D | \neg E)$.
This is known as the Type I or family-wise error rate. To calculate the 
false detection rate ($\fdr$), one must compare $\alpha$ to the statistical power,
the probability of a detection when there is an effect, denoted $W = \Pr(D | E)$.
Both these values must be scaled by the rate at which effects occur, denoted $b$.
Using the notation outlined above, the false detection rate is
\begin{equation}
  \fdr = \frac{(1 - b)\alpha}{(1 - b)\alpha + bW}.
  \label{eq:fdr}
\end{equation}

In group polarization, $E$ represents the group polarization \emph{effect}, and
$\neg E$ represents its absence. Simple consensus is one way for group polarization
to be absent, i.e., simple consensus is one form of $\neg E$. The presence of
an effect $E$ is independent of whether an effect was \emph{detected}, 
represented by $D$ if it was detected and $\neg D$ if not. 

The false discovery rate is then the Type I error rate relative to the rate of
positive discoveries, true or false.
The group polarization studies are journal articles that present
the results of multiple experiments to support some hypothesis. To organize
the presentation of false detection rates across the ten studies in our
analysis, we first will organize false detection rates by experiment, where
we created `ExperimentID` tags for each of the sixty experiments across ten
studies. We also created `StudyID` tags for each of the ten journal articles
with the key `{Author}{Year}`, e.g., \texttt{Schkade2010} for Schkade, et al.,
(2010) \cite{Schkade2010}. 

We calculate $\alpha = \Pr(D | \neg E)$ through simulation for experimental
conditions, 
and use optimistic values of
$b=0.1$~\cite{Ioannidis2005,McElreath2015} and $W=0.8$~\cite{Smaldino2016NatSel}
in the main text and in the compare with more pessimistic, yet realistic, estimates
of these values, as well as even more optimistic estimates. 


\subsubsection{Estimating and limiting false detection rates}

To calculate the false detection rate we begin by simulating 
pre- and post-deliberation 
opinions by drawing $N_e$ samples from normal latent opinion
distributions that generated spurious group polarization, i.e., with
parameters labelled $\nu_e$ found in the hillclimbing step. Binning these
random values simulates observations. $N_e$ is the number
of participants in experimental condition $e$. Each trial $i$ of $N_e$
simulated pre- and post-deliberation observations is then inspected to see
if spurious group polarization occurred in the simulation. Spurious group
polarization is determined by first inferring the latent parameters of each
simulated experimental trial by fitting an ordered probit model~\cite{Liddell2018}. 
The inferred parameters are then used to calculate the effect size for that
trial, Cohen's $d$, defined as~\cite[p. 331]{Liddell2018}
\begin{equation}
  d = \frac{\mupost - \mupre}{\sqrt{\frac{1}{2}(\sigmapost^2 + \sigmapre^2)}}.
  \label{eq:cohens}
\end{equation}
\noindent
The family-wise error rate is, $\alpha_{e} = \Pr(D|\neg E)$, is the 
fraction of trials, 
with $d_{ei} > d^*$ for the $i^\text{th}$ trial for experimental
condition $e$, written
\begin{equation}
  \alpha_e = \frac{1}{N_T} \sum_i^{N_T} \Theta(d_{ei}; d^*),
  \label{eq:study_aggregate}
\end{equation}
\noindent
where
\[
  \Theta(d_{ei}; d^*) = 
    \begin{cases}
      1 & \text{ if } d_{ei} > d^* \\
      0 & \text{ otherwise.}
    \end{cases}
\]
and $N_T = 1000$ is the number of trials. For simplicity we assume that the sign of
$d_{ei}$ is positive, meaning a shift to greater extremisim. It is possible that
$d_{ei}$ could be negative, indicating a shift to lesser extremity. Indeed we
observe many simulations with a shift to lesser extremity. For now we do not
include these as indicating a ``significant'' effect, which we return to in the
Analysis below.

To calculate the family-wise error rate
for a study, $s$ which contains several experimental conditions, $e$,
\begin{equation}
  \alpha_s = \frac{1}{|s|} \sum_{e \in s} \alpha_e
\end{equation}
\noindent
The mean across all results identified as plausibly spurious is 
\begin{equation}
  \alpha_\mathrm{all} = \frac{1}{|E|} \sum_{e \in E} \alpha_e.
\end{equation}
We finally may calculate the false discovery rate by plugging this $\alpha$
into Equation~\ref{eq:fdr} and assuming empirically-motivated values for 
base rate, $b$, and statistical power, $W$, as well as more pessimistic and
optimistic settings for base rate and power.

We close our Analysis by inverting this procedure to find what significance value
$d^*$ is necessary to achieve a low family-wise error rate $\alpha_e = 0.05$ for
experimental condition, $e$. To do this we first calculate $\alpha_e(d^*)$ for a
sufficiently wide range of $d^*$. We then find the lowest $d^*$ for which
$\alpha_e(d^*) \leq 0.05$.

