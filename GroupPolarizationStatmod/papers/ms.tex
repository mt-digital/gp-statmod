% Author: Matthew Turner

% \documentclass[11pt,letterpaper]{scrartcl}
\documentclass[letterpaper,man,natbib]{apa6}
% \documentclass[11pt]{report}
% \documentclass{report}
% \documentclass{book}
\usepackage[bookmarks, hidelinks]{hyperref}
\usepackage{amssymb,amsmath}
% \usepackage{fullpage}
\usepackage{tabulary}
\usepackage{tabularx}
\usepackage{float}
% \usepackage[margin=1.00in]{geometry}
% \usepackage[margin=0.90in]{geometry}

\usepackage{caption}
\usepackage{booktabs}
\usepackage{pslatex}
\usepackage{apacite}
\usepackage{authblk}
\usepackage{subcaption}
\usepackage{pgfplots}
\usepackage{wrapfig}
\usepackage[english]{babel}
\usepackage{lmodern}
% \usepackage{setspace}
% \doublespace
% \usepackage{url}
\usepackage{bigfoot}
\usepackage[export]{adjustbox}
% \setlength\intextsep{0pt}

\usepackage{graphicx}

\title{Group polarization is often a mirage caused by inappropriate statistical analyses}
\shorttitle{Group polarization or simple consensus?}
\author[1,2]{{Matthew A.~Turner}}
\affil[1]{Department of Earth System Science, Stanford University}
\affil[2]{Division of Social Sciences, Stanford Doerr School of Sustainability, Stanford University}

\author[3,4,5]{{Paul E.~Smaldino}}
\affil[3]{Cognitive and Information Sciences, University of California, Merced}
\affil[4]{Center for Advanced Study in the Behavioral Sciences, Stanford University}
\affil[5]{Santa Fe Institute}

\abstract{
  Group polarization is the observed tendency of
  like-minded groups to arrive at a more extreme consensus opinion than
  the average opinion after discussing some topic of interest.
  Clearly it is important to understand such a phenomenon to understand broader
  problematic trends towards increased polariztion across society as a whole.
  Unfortunately, recent analyses have demonstrated that this measurement and
  analysis approach can result in false positive detections group polarization
  from failing to account for floor/ceiling effects induced when using ordinal scales to
  measure continuously-distributed latent psychological variables. 
  Here we demonstrate that many high profile detections
  of group polarization are plausibly false
  due to these floor/ceiling effects using generative modeling seeded by 
  reported group polarization values and parameters. After we identify which
  reported detections of group polarization are plausibly false detections, we further
  investigate whether the latent constant mean opinion and pre- and post-discussion
  opinion variances can fool the statistical tests used in each study to 
  generate false positives. Finally, we evaluate whether a more appropriate
  Bayesian cumulative normal statistical model can more accurately distinguish
  group polarization from simple consensus. Group polarization continues to 
  pose a problem for psychological theory with an ever-expanding set of potential theoretical
  explanations. A search for explanations is pointless, however, without sound
  evidence. We believe this work will provide a sturdier analytical foundation for 
  future group-polarization research.
}


\begin{document}
\maketitle
\setlength{\parindent}{15pt}
% \maketitle


\section{Introduction}


\subsection{Group polarization}

\subsection{Statistical inference in opinion change}

2-3 par: the right way; what GP studies do; why it's a problem (N-1 sents) and how
to fix it (1 sent)

When 


\begin{itemize}
  \item 
    Group polarization studies use null-hypothesis testing that implicitly
    assume (1) \emph{opinions} (or other equivalent latent psychological variables
    such as \emph{attitudes} or \emph{beliefs}) are continuously-valued, and
    (2) 
  \item
    The exact null-hypothesis testing procedure varies between studies $t$-tests, ANOVA, etc. 
\end{itemize}

Recently, problems with using have been made clear~\citep{Liddell2018}.
The way to fix this problem going forward is to use a thresholded-normal 
statistical model for group polarization data, or other related statistical 
model that accounts for opinion binning~\citep[Ch. 23]{Liddell2018,KruschkeDBDA}.

\subsection{Study selection and overview}

Studies were selected to be either influential papers, based on citations or
author prominence, that test either examine
group polarization in political opinions, and to be representative of investigations
of four primary group polarization explanations extant in the literature.
At the time of paper selection we did not examine the statistical methods in detail beyond
checking that they indeed used 

Because of the difficulties casting individual research articles into our modeling
framework, and the central task of creating the model and analyzing this first
batch of studies with it, we limit our analysis to 10 studies. It is problem
enough that these 10 prominent studies have the problems we demonstrate they
plausibly do. Problems among these 10 suggest the problem may be more wide spread. 
When we do more analyses
they will be aided by improved data management systems for applying this model to 
additional studies (eventually outside of group polarization).

Statistical problems among these studies are not limited to a mismatch between
measurement and statistical procedures, which requires the detailed treatment
we provide in this paper. Other problems are more pedestrian.
One major issue shared by several studies
is the use of unpaired $t$-tests to detect group polarization. Since the same
group has their opinions measured twice over time, the opinions are correlated
and so a paired $t$-test would be required for test assumptions to be met 
(assuming all other paired $t$-test assumptions were met, 
which of course they are not, which is why our paper here is necessary).
In some cases the reported shifts were ``significant'', 
but not truly ``group polarization'' shifts, i.e., 
shifts from initially biased group opinions
to more extreme group opinions biased in the same direction; instead shifts 
were from a neutral initial group opinion to a biased group opinion 
(as in, e.g.,~\citet{Abrams1990} Uncategorized2), or from
an opinion biased in one direction (as in, e.g.,~\citet{Abrams1990} Categorized1,
Categorized2, AND OTHERS???). In some results of \citet{Freidkin1999a}, 
shift values are reported with their
standard deviations---but the shift standard deviations are greater than the
shifts themselves, indiciating that zero shift is included in the observed
shift confidence interval. This is \emph{prima facie} a negative result that
should not have been counted as evidence of group polarization. 

Some group polarization studies do not measure continuous latent opinions using
a metric scale, and so are not affected by the floor and ceiling effects that
undermine a good number of experimental results, as we examine here. There may,
however, be other problems, such as non-group polarization shifts and the use
of unpaired $t$-tests when paired tests are required.

\subsection{Research overview}

In this paper we analyze whether published statistical detections of group 
polarization obtained through null-hypothesis statistical tests
are plausibly false because they can be explained equally well by simple consensus
where the mean group opinion does not change over time, but opinion variance does.
This analysis requires several preliminary steps. 
First we formally develop our generative model of simple consensus and group 
polarization in terms of changes in group opinion mean and variance from 
pre- to post-deliberation. Then we developed a search algorithm for identifying 
which, if any, simple consensus scenarios with 
constant latent mean and pre- and post-deliberation opinion variances 
could generate published observations of group polarization. To apply this model
and algorithm to published results, we developed an interactive web application
for data input and to test which different potential simple consensus scenarios 
could generate group polarization opinion shifts. After identifying plausibly false
detections of group polarization we examine whether the identified simple
consensus scenario (parameters?) can indeed generate data to fool the 
paper's null-hypothesis tests into thinking the data actually support the inference
that simple consensus did not occur. Finally, we examine whether, in these same
plausibly false detections, a Bayesian thresholded-normal statistical model
will accurately identify simple consensus instead of falsely detecting group
polarization.

\section{Model and Data}

Our model represnts the generative process of finding different forms of group consensus
in group polarization experiments, and the process of measuring that consensus using
an ordinal measurement scale, e.g.\ a Likert-type scale. This model considers two
relevant forms of consensus: \emph{simple consensus} and \emph{group polarization}.
Either process is \emph{consensus} since opinion variance among the group 
decreases as individuals come to agree more than they did previously. 
Group polarization experiments are designed to foster group consensus
it is not necessary to consider clustering, negative influence, or other
processes known to generally occur when opinions within a group significantly
diverge~\citep{Turner2018}.

We collected group polarization data from ten empirical studies with sixty constituent
experimental treatments across all studies, specifically the opinion shift and pre- and
post-deliberation mean opinions, and the number of participants
in each treatment (\citet{Burnstein1975} did not report pre- and post-deliberation
means, only the shift in means). We did not include experimental treatments that did not 
yield a positive detection of group polarization since we are only estimating
the rate of plausible false positive detections of group polarization. 
We then created new data that indicated whether a reported. 

In two cases, reported opinion and shift values were converted from an ordinal
scale to a percentage scale, specifically \citet{Friedkin1999a} and \citet{Krizan2007}.
The authors in these papers did report the original ordinal measurement scale,
so we converted the means back to the orignial measurement scale.
In a few cases from \citet{Friedkin1999a} we did not need to apply our 
model to the reported data because we could determine a reported opinion shift was 
\emph{prima facie} false since reported opinion shift magnitudes
were smaller than the standard deviation in shifts (see Table 1, p. 868, \emph{ibid}),
despite Friedkin's claim that regressions (not clearly defined) on all shifts 
yielded $p < 0.05$~\cite{Kruschke2018c}.


\subsection{Latent opinions, ordinal measurements}

Our analysis relies on two important formal concepts: latent opinions and their 
measurement on an ordinal scale. \emph{Latent opinions} are unobservable psychological,
continuous-valued variables that are somehow represented and stored in the
brain-body. 
When researchers use continuous statistical models to infer 
that group polarization occurred (which all ten considered here do), 
they are implicitly assuming that latent opinions are themselves continuous.
We can only measure behaviors in group polarization experiments,
specifically the behavior of participants marking one ordinal value or another
to indicate their opinion---a process that is not well understood in itself,
but which is unimportant for our analyses. \emph{Ordinal measurements} (e.g.\
a Likert or other discrete-valued opinion scale) of latent opinions force
participants to appropriately bin their opinion in one of several categories
representing the valence and extremity of their opinion, e.g., on a
7-point Likert scale -3 could indcate
``strongly disagree'', +3 could be ``strongly agree'', 0 would be neutral, and 
intermediate values are less extreme opinions of either valence.

All ten studies analyzed here used an ordinal scale, but group polarization experiments seem to
lack any systematicity in experimental design beyond that. (EXAMPLES)

\subsection{Simple consensus and group polarization are statistical patterns}

\subsection{Analytical approach}

The goal of this study is to understand if published detections of group polarization are
plausibly false. 

Therefore we designed a series of tests to understand if, and to what degree,
published results are false positive detections of group polarization that
are at least as well described by a simple-consensus model.



\subsubsection{Identifying parameters that plausibly generate false detections of group polarization}

\subsubsection{$t$-tests of generated simple-consensus data}

\subsubsection{Bayesian ordered-probit models of generated simple-consensus data}

\section{Analysis}

\subsection{Plausible parameters for false positives}

We found that X\% of group polarization detections across all studies were 
plausibly false positives (Table~\ref{tab:XXXXX}).

\subsection{Do $t$-tests accurately detect simple consensus?}

We found that in X\% of generated simple consensus data led to false positive
detections 

When simulated data reliably result in false positive $t$-test results (where
the $t$-test $p \le 0.1$) this strengthens the case that


\subsection{Do Bayesian ordered-probit models accurately detect simple consensus?}

We found that in many cases Bayesian ordered-probit models avoid falsely identifying simple
consensus as group polarization. This supports the claim that it is the choice of statistical
procedure, including the failure to account for the ordinal nature of, and variance within,
opinion measuremenets in group polarization experiments that leads to false
positives, not anything intrinsic to the phenomenon of group polarization.


\section{Discussion}

We showed that many prominent detections of group polarization are plausibly
false, and that future studies \emph{must} use ordered-probit statistical
models and estimate highest-density or confidence intervals, not point-estimates 
and significance values~\cite{Meehl1997}.

Due to a lack of data sharing in general, and often from decades-old papers, and a persistent 
failure of group polarization work to provide opinion variance information, 
it is impossible to know whether an historical detection of group polarization
is a true or false positive. 

This problem is made all the worse by the lack of a consistent experimental design for group
polarization experiments. This makes it impossible to systematically account for
additional sources of vairance known to lead to (orthogonal) over-estimates
of effect sizes~\cite{Yarkoni2021}. This is due to the rush to explain why 
group polarization occurs using weak theory~\cite{Cartwright1971,Cartwright1973} and makes
group polarization meta-analyses (e.g. Brown, 1986; and Isenberg, 1986)\nocite{Brown1986,Isenberg1986}
unreliable~\cite{Meehl1990,Meehl1997}.

Group polariztion is an essential social phenomenon that could be a major driver of
progress-halting political polarization more broadly. If echo chambers of like-minded
individuals tend to become radicalized it is essential to measure by how much
in different contexts, and to develop rigorous theoretical explanations of
this phenomenon. It appears, unfortunately, that decades of group polarization
research may have published many plausibly false inferences of the
occurrence of group polarization, as demonstrated by our simple consensus
model of false group polarization detection. As mentioned, a lack of
consistency and failure to account for additional sources of variance could
be two additional ways published research is underpowered, and associated
theoretical insights undermined. It may not be easy to get the group polarization field to 
consolidate experimental designs and use appropriate statistical procedures,
but it is a straightforward solution. Indeed, combining inter-study consistency and statistical
rigor is the only way to a reliable understanding of group polarization.

\bibliographystyle{apacite}

\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}

\bibliography{/Users/mt/workspace/Writing/library.bib}

\appendix

\section{Notes on studies}

Different group polarization studies have different goals, and measure shifts
in a variety of ways, using different (or no) statistical tests to confirm
the presence of group polarization opinion shifts. This supplemental section
reviews the ten papers one-by-one and explains how group 
polarization was measured in each paper. I also explain how I decided which
group polarization detections I included from each paper, and how we decided
if they were plausibly false positives, including whether we needed to 

\paragraph{\citet{Abrams1990}} 

\paragraph{\citet{Burnstein1973}}

\paragraph{\citet{Hogg1990}} It is not clear if there are indeed significant
pre-post deliberation differences reported in this paper. The Results are a
tangle of estimates of effect sizes and diagnostic statistical measures. It
is also not clear that they actually report the pre- and post-deliberation
opinions; it appears they only report the shifts. ``Opinion Difference
Measures'' does not seem to include a test that the shifts themselves are
reliable. I decided to count a finding from this study as a false positive if
the model generated pre- and post-deliberation mean opinions within an
absolute error of 0.01 of the reported value. The argument seems to be that
since they are testing social ``frame[s] of reference'' (Table 1) that it is
not necessary to show that group polarization itself occurs, only that the
various experimental treatments yield different shifts---but if the shifts
themselves are insignificant, then what is being measured, exactly?


\end{document}
