% % Author: Matthew Turner

% % \documentclass[11pt,letterpaper]{scrartcl}
% % \documentclass[letterpaper,man,natbib]{apa6}
% \documentclass[11pt,letterpaper]{amsart}
% % \documentclass[11pt, letterpaper]{article}
% % \usepackage{natbib}
% % \documentclass[11pt]{report}
% % \documentclass{report}
% % \documentclass{book}
% \usepackage[bookmarks, hidelinks]{hyperref}
% \usepackage{amssymb,amsmath}
% % \usepackage{fullpage}
% \usepackage{tabulary}
% \usepackage{tabularx}
% \usepackage{float}
% % \usepackage[margin=1.00in]{geometry}
% \usepackage[margin=0.90in]{geometry}

% % \usepackage{fontspec} 

% % \usepackage{xunicode}
% % \usepackage{xltxtra}
% % \defaultfontfeatures{Mapping=tex-text}
% % \setmainfont[Ligatures={Common}, Numbers={OldStyle}, Variant=01]{Hoefler Text}
% % \setromanfont[Ligatures={Common}, Numbers={OldStyle}, Variant=01]{Hoefler Text}
% % \setmonofont[Scale=0.95]{PT Sans}
% \usepackage{caption}
% \usepackage{apacite}
% \usepackage{booktabs}
% \usepackage{pslatex}
% \usepackage{subcaption}
% \usepackage{pgfplots}
% \usepackage{wrapfig}
% \usepackage[english]{babel}
% \usepackage{lmodern}
% % \doublespace
% % \usepackage{url}
% \usepackage{bigfoot}
% \usepackage[export]{adjustbox}
% % \setlength\intextsep{0pt}

% \usepackage{graphicx}

\documentclass[12pt, letterpaper]{article}
\usepackage{fontspec} 
\usepackage{marvosym}
\usepackage{setspace}
\usepackage{longtable}
% DOCUMENT LAYOUT
% \usepackage{geometry} 
\usepackage[margin=0.90in]{geometry}
\usepackage{lineno}
\usepackage[usenames,dvipsnames]{xcolor}
% \geometry{letterpaper, textwidth=5.5in, textheight=8.5in, marginparsep=7pt, marginparwidth=.6in}
% \setlength\parindent{0in}

\definecolor{linenocolor}{gray}{0.6}
\renewcommand\thelinenumber{\color{linenocolor}\arabic{linenumber}}
% \calclayout %centers text on page.

% \renewcommand{\thesubsection}{\arabic{subsection}.}
% FONTS
% \usepackage[usenames,dvipsnames]{color}
% \usepackage{xunicode}
% \usepackage{xltxtra}
\defaultfontfeatures{Mapping=tex-text}
% \setromanfont[Ligatures={Common}, Variant=01]{Hoefler Text}
% \setromanfont[Ligatures={Common}, Variant=01]{Palatino}
\setromanfont[Ligatures={Common}, Variant=01]{Baskerville}
% \setromanfont[Ligatures={Common}, Variant=01]{Plantagenet Cherokee}
% \setmainfont [BoldFont={STIX Two Text SemiBold}, Ligatures={Common},
% Numbers={OldStyle}, Variant=01]{STIX Two Text}
% \setmainfont [BoldFont={STIX Two Text SemiBold}, Ligatures={Common},
% Numbers={OldStyle}, Variant=01]{STIX Two Text}
% \setromanfont [Ligatures={Common},
% Numbers={OldStyle}, Variant=01]{STIX Two Text}
\setmonofont[Scale=0.95]{PT Sans}

% ---- CUSTOM COMMANDS
% \chardef\&="E050
\newcommand{\html}[1]{\href{#1}{\scriptsize\textsc{[html]}}}
\newcommand{\pdf}[1]{\href{#1}{\scriptsize\textsc{[pdf]}}}
\newcommand{\doi}[1]{\href{#1}{\scriptsize\textsc{[doi]}}}
\newcommand{\lurl}[1]{\href{#1}{\scriptsize\textsc{[link]}}}

\newcommand{\githubsymbol}{\raisebox{-2pt}{\includegraphics[height=10.5pt]{github}}}
\newcommand{\twittersymbol}{\raisebox{-2pt}{\includegraphics[height=10.5pt]{twitter}}}
\newcommand{\monitorsymbol}{\raisebox{-2pt}{\includegraphics[height=10.5pt]{monitor}}}

% ---- MARGIN YEARS
\usepackage{marginnote}
\newcommand{\amper{}}{\chardef\amper="E0BD }
\newcommand{\years}[1]{\marginnote{\small #1}}
\renewcommand*{\raggedleftmarginnote}{}
\setlength{\marginparsep}{7pt}
\reversemarginpar

% HEADINGS
\usepackage{sectsty} 
\usepackage[normalem]{ulem} 
\sectionfont{\upshape\Large}
\subsectionfont{\scshape\normalsize} 
\subsubsectionfont{\upshape\large} 


% PDF SETUP
% ---- FILL IN HERE THE DOC TITLE AND AUTHOR
\usepackage[%dvipdfm, 
bookmarks, colorlinks, breaklinks, 
% ---- FILL IN HERE THE TITLE AND AUTHOR
	pdftitle={Matthew A. Turner - Vita},
	pdfauthor={Matthew A. Turner, PhD},
	pdfproducer={https://mt.digital}
]{hyperref}  
\hypersetup{linkcolor=blue,citecolor=blue,filecolor=black,urlcolor=MidnightBlue} 

\usepackage{apacite}
\usepackage{booktabs}
\usepackage{authblk}

\begin{document}

\title{Group polarization is often a mirage caused by inappropriate statistical analyses}
% \shorttitle{Group polarization or simple consensus?}
\author[1,2]{{Matthew A.~Turner}}
\affil[1]{Department of Earth System Science, Stanford University}
\affil[2]{Division of Social Sciences, Stanford Doerr School of Sustainability, Stanford University}

\author[3,4,5]{{Paul E.~Smaldino}}
\affil[3]{Cognitive and Information Sciences, University of California, Merced}
\affil[4]{Center for Advanced Study in the Behavioral Sciences, Stanford University}
\affil[5]{Santa Fe Institute}



\maketitle
% \setlength{\parindent}{15pt}
% \maketitle

\spacing{1.5}

\linenumbers
\modulolinenumbers[5]

\begin{abstract}
  \noindent
  Group polarization is the observed tendency of
  like-minded groups to arrive at a more extreme consensus opinion than
  the average opinion after discussing some topic of interest.
  Clearly it is important to understand such a phenomenon to understand broader
  problematic trends towards increased polariztion across society as a whole.
  Unfortunately, recent analyses have demonstrated that this measurement and
  analysis approach can result in false positive detections group polarization
  from failing to account for floor/ceiling effects induced when using ordinal scales to
  measure continuously-distributed latent psychological variables. 
  Here we demonstrate that many high profile detections
  of group polarization are plausibly false
  due to these floor/ceiling effects using generative modeling seeded by 
  reported group polarization values and parameters. After we identify which
  reported detections of group polarization are plausibly false detections, we further
  investigate whether the latent constant mean opinion and pre- and post-discussion
  opinion variances can fool the statistical tests used in each study to 
  generate false positives. Finally, we evaluate whether a more appropriate
  Bayesian cumulative normal statistical model can more accurately distinguish
  group polarization from simple consensus. Group polarization continues to 
  pose a problem for psychological theory with an ever-expanding set of potential theoretical
  explanations. A search for explanations is pointless, however, without sound
  evidence. We believe this work will provide a sturdier analytical foundation for 
  future group-polarization research.
\end{abstract}

\section{Introduction}


\subsection{Group polarization}

\subsection{Statistical inference in opinion change}

2-3 par: the right way; what GP studies do; why it's a problem (N-1 sents) and how
to fix it (1 sent)

When 


\begin{itemize}
  \item 
    Group polarization studies use null-hypothesis testing that implicitly
    assume (1) \emph{opinions} (or other equivalent latent psychological variables
    such as \emph{attitudes} or \emph{beliefs}) are continuously-valued, and
    (2) 
  \item
    The exact null-hypothesis testing procedure varies between studies $t$-tests, ANOVA, etc. 
\end{itemize}

Recently, problems with using have been made clear~\cite{Liddell2018}.
The way to fix this problem going forward is to use a thresholded-normal 
statistical model for group polarization data, or other related statistical 
model that accounts for opinion binning~\cite[Ch. 23]{Liddell2018,KruschkeDBDA}.

\subsection{Study selection and overview}

Studies were selected to be either influential papers, based on citations or
author prominence, that test either examine
group polarization in political opinions, and to be representative of investigations
of four primary group polarization explanations extant in the literature.
At the time of paper selection we did not examine the statistical methods in detail beyond
checking that they indeed used 

Because of the difficulties casting individual research articles into our modeling
framework, and the central task of creating the model and analyzing this first
batch of studies with it, we limit our analysis to 10 studies. It is problem
enough that these 10 prominent studies have the problems we demonstrate they
plausibly do. Problems among these 10 suggest the problem may be more wide spread. 
When we do more analyses
they will be aided by improved data management systems for applying this model to 
additional studies (eventually outside of group polarization).

Statistical problems among these studies are not limited to a mismatch between
measurement and statistical procedures, which requires the detailed treatment
we provide in this paper. Other problems are more pedestrian.
One major issue shared by several studies
is the use of unpaired $t$-tests to detect group polarization. Since the same
group has their opinions measured twice over time, the opinions are correlated
and so a paired $t$-test would be required for test assumptions to be met 
(assuming all other paired $t$-test assumptions were met, 
which of course they are not, which is why our paper here is necessary).
In some cases the reported shifts were ``significant'', 
but not truly ``group polarization'' shifts, i.e., 
shifts from initially biased group opinions
to more extreme group opinions biased in the same direction; instead shifts 
were from a neutral initial group opinion to a biased group opinion 
(as in, e.g.,~\citeA{Abrams1990} Uncategorized2), or from
an opinion biased in one direction (as in, e.g.,~\citeA{Abrams1990} Categorized1,
Categorized2, AND OTHERS???). In some results of \citeA{Freidkin1999a}, 
shift values are reported with their
standard deviations---but the shift standard deviations are greater than the
shifts themselves, indiciating that zero shift is included in the observed
shift confidence interval. This is \emph{prima facie} a negative result that
should not have been counted as evidence of group polarization. 

Some group polarization studies do not measure continuous latent opinions using
a metric scale, and so are not affected by the floor and ceiling effects that
undermine a good number of experimental results, as we examine here. There may,
however, be other problems, such as non-group polarization shifts and the use
of unpaired $t$-tests when paired tests are required.

\subsection{Research overview}

In this paper we analyze whether published statistical detections of group 
polarization obtained through null-hypothesis statistical tests
are plausibly false because they can be explained equally well by simple consensus
where the mean group opinion does not change over time, but opinion variance does.
This analysis requires several preliminary steps. 
First we formally develop our generative model of simple consensus and group 
polarization in terms of changes in group opinion mean and variance from 
pre- to post-deliberation. Then we developed a search algorithm for identifying 
which, if any, simple consensus scenarios with 
constant latent mean and pre- and post-deliberation opinion variances 
could generate published observations of group polarization. To apply this model
and algorithm to published results, we developed an interactive web application
for data input and to test which different potential simple consensus scenarios 
could generate group polarization opinion shifts. After identifying plausibly false
detections of group polarization we examine whether the identified simple
consensus scenario (parameters?) can indeed generate data to fool the 
paper's null-hypothesis tests into thinking the data actually support the inference
that simple consensus did not occur. Finally, we examine whether, in these same
plausibly false detections, a Bayesian thresholded-normal statistical model
will accurately identify simple consensus instead of falsely detecting group
polarization.

\section{Model and Data}

Our model represnts the generative process of finding different forms of group consensus
in group polarization experiments, and the process of measuring that consensus using
an ordinal measurement scale, e.g.\ a Likert-type scale. This model considers two
relevant forms of consensus: \emph{simple consensus} and \emph{group polarization}.
Either process is \emph{consensus} since opinion variance among the group 
decreases as individuals come to agree more than they did previously. 
Group polarization experiments are designed to foster group consensus
it is not necessary to consider clustering, negative influence, or other
processes known to generally occur when opinions within a group significantly
diverge~\cite{Turner2018}.

We collected group polarization data from ten empirical studies with sixty constituent
experimental treatments across all studies, specifically the opinion shift and pre- and
post-deliberation mean opinions, and the number of participants
in each treatment (\citeA{Burnstein1975} did not report pre- and post-deliberation
means, only the shift in means). We did not include experimental treatments that did not 
yield a positive detection of group polarization since we are only estimating
the rate of plausible false positive detections of group polarization. 
We then created new data that indicated whether a reported. 

In two cases, reported opinion and shift values were converted from an ordinal
scale to a percentage scale, specifically \citeA{Friedkin1999a} and \citeA{Krizan2007}.
The authors in these papers did report the original ordinal measurement scale,
so we converted the means back to the orignial measurement scale.
In a few cases from \citeA{Friedkin1999a} we did not need to apply our 
model to the reported data because we could determine a reported opinion shift was 
\emph{prima facie} false since reported opinion shift magnitudes
were smaller than the standard deviation in shifts (see Table 1, p. 868, \emph{ibid}),
despite Friedkin's claim that regressions (not clearly defined) on all shifts 
yielded $p < 0.05$~\cite{Kruschke2018c}.


\subsection{Latent opinions, ordinal measurements}

Our analysis relies on two important formal concepts: latent opinions and their 
measurement on an ordinal scale. \emph{Latent opinions} are unobservable psychological,
continuous-valued variables that are somehow represented and stored in the
brain-body. 
When researchers use continuous statistical models to infer 
that group polarization occurred (which all ten considered here do), 
they are implicitly assuming that latent opinions are themselves continuous.
We can only measure behaviors in group polarization experiments,
specifically the behavior of participants marking one ordinal value or another
to indicate their opinion---a process that is not well understood in itself,
but which is unimportant for our analyses. \emph{Ordinal measurements} (e.g.\
a Likert or other discrete-valued opinion scale) of latent opinions force
participants to appropriately bin their opinion in one of several categories
representing the valence and extremity of their opinion, e.g., on a
7-point Likert scale -3 could indcate
``strongly disagree'', +3 could be ``strongly agree'', 0 would be neutral, and 
intermediate values are less extreme opinions of either valence.

All ten studies analyzed here used an ordinal scale, but group polarization experiments seem to
lack any systematicity in experimental design beyond that. (EXAMPLES)

\subsection{Simple consensus and group polarization are statistical patterns}

\subsection{Analytical approach}

The goal of this study is to understand if published detections of group polarization are
plausibly false. 

Therefore we designed a series of tests to understand if, and to what degree,
published results are false positive detections of group polarization that
are at least as well described by a simple-consensus model.



\subsubsection{Identifying parameters that plausibly generate false detections of group polarization}

\subsubsection{$t$-tests of generated simple-consensus data}

\subsubsection{Bayesian ordered-probit models of generated simple-consensus data}

\section{Analysis}

\subsection{Plausible parameters for false positives}

We found that X\% of group polarization detections across all studies were 
plausibly false positives (Table~\ref{tab:XXXXX}).

\spacing{1.0}
\input{plausibleFP-table.tex}
\spacing{1.5}

\subsection{Do $t$-tests accurately detect simple consensus?}

We found that in X\% of generated simple consensus data led to false positive
detections 

When simulated data reliably result in false positive $t$-test results (where
the $t$-test $p \le 0.1$) this strengthens the case that reported detections
of a false group polarization detection.


\spacing{1.0}
% \input{t-test-table.tex}
\input{t-test-table-custom.tex}
\spacing{1.5}

\subsection{Do Bayesian ordered-probit models accurately detect simple consensus?}

We found that in many cases Bayesian ordered-probit models avoid falsely identifying simple
consensus as group polarization. This supports the claim that it is the choice of statistical
procedure, including the failure to account for the ordinal nature of, and variance within,
opinion measuremenets in group polarization experiments that leads to false
positives, not anything intrinsic to the phenomenon of group polarization.


\section{Discussion}

We showed that many prominent detections of group polarization are plausibly
false, and that future studies \emph{must} use ordered-probit statistical
models and estimate highest-density or confidence intervals, not point-estimates 
and significance values~\cite{Meehl1997}.

Due to a lack of data sharing in general, and often from decades-old papers, and a persistent 
failure of group polarization work to provide opinion variance information, 
it is impossible to know whether an historical detection of group polarization
is a true or false positive. 

This problem is made all the worse by the lack of a consistent experimental design for group
polarization experiments. This makes it impossible to systematically account for
additional sources of vairance known to lead to (orthogonal) over-estimates
of effect sizes~\cite{Yarkoni2021}. This is due to the rush to explain why 
group polarization occurs using weak theory~\cite{Cartwright1971,Cartwright1973} and makes
group polarization meta-analyses (e.g. Brown, 1986; and Isenberg, 1986)\nocite{Brown1986,Isenberg1986}
unreliable~\cite{Meehl1990,Meehl1997}.

Group polariztion is an essential social phenomenon that could be a major driver of
progress-halting political polarization more broadly. If echo chambers of like-minded
individuals tend to become radicalized it is essential to measure by how much
in different contexts, and to develop rigorous theoretical explanations of
this phenomenon. It appears, unfortunately, that decades of group polarization
research may have published many plausibly false inferences of the
occurrence of group polarization, as demonstrated by our simple consensus
model of false group polarization detection. As mentioned, a lack of
consistency and failure to account for additional sources of variance could
be two additional ways published research is underpowered, and associated
theoretical insights undermined. It may not be easy to get the group polarization field to 
consolidate experimental designs and use appropriate statistical procedures,
but it is a straightforward solution. Indeed, combining inter-study consistency and statistical
rigor is the only way to a reliable understanding of group polarization.

\bibliographystyle{apacite}

\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}

\bibliography{/Users/mt/workspace/Writing/library.bib}

\appendix

\section{Notes on studies}

Different group polarization studies have different goals, and measure shifts
in a variety of ways, using different (or no) statistical tests to confirm
the presence of group polarization opinion shifts. This supplemental section
reviews the ten papers one-by-one and explains how group 
polarization was measured in each paper. I also explain how I decided which
group polarization detections I included from each paper, and how we decided
if they were plausibly false positives, including whether we needed to 

\paragraph{{Abrams1990}} This study is meant to support the presence of the "self-categorization" effect in
social psychology. The paper presents three experiments and their results--Experiment 3 is the group polarization
experiment, designed to show that self-categorization is at work in the emergence of group polarization. Uses high
schoolers aged 16-17 in Hawkes Bay, New Zealand. There are five deliberation topics (p. 112) across two conditions
(p. 113): one condition where participants are made aware of the relationship between their opinions and other group
members' opinions ("Categorized") and another condition where participants were given no information about their
opinion relative to other group members' opinions ("Uncategorized"). It was confusing to me what scale is used for
Table 2 (p. 114), but parsing the "Dependent measures" subsection several times led me to understand the last
paragraph, that "[a]ttitude shift overall was...assessed...on the unconverted +4 to -4 scale."

\paragraph{{Burnstein1973}} First I want to note this study was funded in part by a Guggenheim fellowship (see
note on first page) and the authors acknolwedge the assistance of "vinous" discussions with colleagues at the
University of Provence and the "spirited and intelligent efforts" of two female research assistants. The study
presents the results of two experiments (really one experiment with two pools of participants) that test the
persuasive arguments hypothesis by having students perform the standard group polarization experiment with five CDQ
discussion items, items A,B,E,F, and H (A,B risk-inducing; E, F caution-inducing; H neutral). Experiment 1 used 149
male intro psych students and Experiment 2 used 76 male intro psych students from the University of Michigan. There
are three conditions across the two experiments--one hypothesized to show an emergent shift and the other two
hypothesized to not show a shift. None of the conditions allowed participants to know each others' ratings exactly,
the participants only deliberated the various topics. In the one hypothesized to show a shift, participants were
instructed to argue for their position. In the other two, participants argued against their position. The "against"
condition was further divided into a case where everyone knew everyone was prompted to argue against their position,
and another sub-condition where participants did not know if others were arguing for or against their true position.
We include only the first condition where participants argued for their position, which is the only condition
expected to show shifts (though others do, but the authors let themselves off the hook saying, "They are curious
findings and defy straightforward explanation...we will merely note their occurrence and not venture to speculate as
to their cause"). Thus, in our table we have conditions A, B, E, F, and H. Even though H is supposed to not show a
shift, it does in this study, which is possibly further evidence that this study merely shows participant consensus
formation, not opinion shifts.

\paragraph{{Burnstein1975}} In this study, Burnstein and Vinokur ran an experiment to evaluate their theory
that generating persuasive arguments are necessary for group polarization to occur. The authors design a study with
three conditions, one Experimental and two Control, where participants (60 male students from intro psych at the
University of Michigan) first answered four CDQ items that typically generate shifts towards more risk, then, in the
Experimental condition, saw group members' responses and produced arguments that might support the positions of
other participants. In the first control condition, Condition II in the paper and Control1 in our table,
participants were exposed to others' opinions, but does not get to reflect on their opinions or give arguments. In
condition III, the second control condtition (we call Control2) participants do not get to know other participants'
opinions at all. Thus, Burnstein and Vinokur hypothesize that significant shifts will only occur in Condition I, the
Experimental condition. We thus only include Condition I in our analysis. The authors only report the shift, meaning
we are free to choose any pre- and post-deliberation value to see if the null hypothesis is actually plausible.  

\paragraph{{Schkade2010}} Friedkin uses three CDQ topics, "Sports", "School", and "Surgery" that use a
20-point CDQ risk scale in fractional units, 0.05, 0.10, ..., 1.00 across three group sizes (2, 3, and 4, which he
calls "dyads", "triads", and "tetrads", which we use in our CaseStudyTag). Friedkin only gives opinion shifts, and
apparently transforms the fractional CDQ responses into percentages, since the shifts range between 5 and 9.
However, closer inspection of Friedkin's results shows that the reported "Choice Shift" is not equal to the
difference between mean final and initial opinions. Friedkin also uses "different" means for pre- and
post-discussion opinions, which seems easy enough to show are really the same by re-organizing terms in the sum.
Friedkin uses two topics in the tetrad condition where responses are in units of money where participants can freely
give any number they choose ("Asbestos" and "Disaster"), and another topic called "Institutions" where participants
rate 13 institutions, which participants rated on a scale from 0 to 100 percent to indicate the degree of trust the
participant has in each institution. We only consider the CDQ topics since these are the ones rated on a discrete
scale. 

\paragraph{{Hogg1990}} It is not clear if there are indeed significant
pre-post deliberation differences reported in this paper. The Results are a
tangle of estimates of effect sizes and diagnostic statistical measures. It
is also not clear that they actually report the pre- and post-deliberation
opinions; it appears they only report the shifts. ``Opinion Difference
Measures'' does not seem to include a test that the shifts themselves are
reliable. I decided to count a finding from this study as a false positive if
the model generated pre- and post-deliberation mean opinions within an
absolute error of 0.01 of the reported value. The argument seems to be that
since they are testing social ``frame[s] of reference'' (Table 1) that it is
not necessary to show that group polarization itself occurs, only that the
various experimental treatments yield different shifts---but if the shifts
themselves are insignificant, then what is being measured, exactly?

\paragraph{{Krizan2007}} Uses five CDQ topics ("scenarios") from Kogan and
Wallach (1967) that typically provide shifts toward more risky decisions.  These are
given in Footnote 2 on p. 194. Krizan and Baron report shifts and mean pre-test
opinions on the risk scale, meaning that shifts are expected to be negative towards
lower odds of success required to take the risk, i.e., the acceptance of more risk.
Krizan and Baron report mean pre-test opinions, but average over all three conditions
for each scenario, then report shifts for each scenario and condition, so we
effectively have no way for an exact match of initial mean to condition x scenario,
where conditions are no-discussion, deliberation without knowledge of an out-group's
opinion, and deliberation with knowledge of an out-group's opinion. We are only
interested in the non-control cases--K\&B found no difference in knowing out-group
opinion, which they say is evidence against the self-categorization hypothesis of group
polarization.  Therefore, we have ten cases we test across five scenarios and two
conditions with our model, and we use the reported initial mean for each scenario
twice--once for each scenario across the two conditions. Post-deliberation opinions are
calculated in the spreadsheet using the reported shifts. K\&B also report the "Overall"
shift, but I find this meaningless since it is across different items with no
statistical control for inter-item variability. I noted that false detections are
plausible in all cases due to the fact that the standard deviations of shifts overwhelm
the shifts themselves, but I will still run the model on these cases to confirm.

\paragraph{{Moscovici1969}} Moscovici and Zavalloni studied whether group
polarization is more general than CDQ by using opinions about political topics. There
were two general topics--attitudes about "Americans" and attitudes about then-president
of France Charles "DeGaulle"--with 12 and 11 specific items, respectively, on which
participants gave opinions and deliberated. Participants deliberated on one of the two
general topics, giving opinions on each item on a seven-point Likert scale, -3 to +3.
The authors reported pre-consensus and post-consensus mean opinions taken over all
participants and items for each of the two general topics I have called the "Americans"
and "DeGaulle" conditions.

\paragraph{{Myers1970}} Myers and Bishop test whether group polarization occurs
in groups with different levels of racial bias determined by a survey that is not also
the deliberation topic. The participants were high school students from western
Michigan. In the experimental condition, social influence occurs through deliberation;
in the control condition there is no deliberation or social influence at all, just
re-tested opinions after a distraction task. Obviously we only report on whether there
was a significant shift in the experimental condition. In both conditions, three groups
were created according to their racial prejudice as measured by a 100-item racial
attitude measurement instrument, called "low", "medium", and "high" prejudice groups.
The participants then gave their opinions, deliberated, and gave opinions again on
eight deliberation topics. Means were taken across items and prejudice group.

\paragraph{{Myers1975}} Myers performs two experiments each with experimental and control conditions for two group types with initial biases. In Experiment 1, Myers creates biased groups by having participants (intro psychology students) read a description of six different teachers, three "bad" professors and three "good" professors, which the participants are assumed to infer through vignettes about the professors. The participants in the experimental condition of Experiment 1 deliberated in small groups after giving their initial opinions on six items a 0-to-10 bad-to-good scale. In Experiment 2, participants were put into conservatively or liberally biased small groups based on their pre-deliberation opinions on six deliberation items regarding womens' rights and the role of women in the family and society. Again, in the experimental group, participants deliberated each item before giving their final opinions on the items. In the control group in each Experiment, there was no deliberation, just some filler task. Curiously, significant shifts were still found in the control groups--it is unclear to us why. Experiment 1 also had participants adjust the bad and good professors' pay to obtain non-ordinal measures of opinion regarding these professors, which we do not consider here. However, as we will discuss in the paper, this does not make it free of problems--like so many other group polarization studies, Myers statistical model does not account for inter-item, inter-group, or inter-participant variability.

\paragraph{{Schkade2010}} Participants in Schkade, Sunstein, and Hastie's study were put into two politically ideologically biased groups based on their geographical location in the US state of Colorado: participants were drawn from Boulder, Colorado (abbreviated CO), to be liberally biased and from Colorado Springs, CO, to be conservatively biased. Participants gave their opinions and discussed three topics related to Global Warming, Civil Unions, and Affirmative Action. Pre- and post-deliberation mean are given in Table 1 (p. 232) for all six (topics x geographies).  

\end{document}
