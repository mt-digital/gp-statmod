[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes and code examples to accompany “If the Null Fits, You Must Omit”",
    "section": "",
    "text": "% Optional: also define argmax"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Notes and code examples to accompany “If the Null Fits, You Must Omit”",
    "section": "Overview",
    "text": "Overview\nHere are some quick notes on the group polarization codebase in R supporting our article, If the Null Fits, You Must Omit. The code is hosted on GitHub.\nTo follow along, please clone the gp-statmod directory.\n$ git clone https://github.com/mt-digital/gp-statmod.git\nChange directories to go to the gp-statmod/docs directory where this document lives:\ncd gp-statmod/docs\nThen you can run the examples in this document, edit them, etc., on your own computer.\nThis tutorial is still valuable if you’re just reading it on the code documentation associated with the journal article.\nYou will learn how the code works that simulates measurements of opinion change in group polarization experiments that test if and by how much group discussion polarizes opinions. I used this modeling system to simulate 57 different group polarization experimental conditions across a corpus of ten journal articles published over several decades, and across social science subdisciplines for my paper with Paul Smaldino, “Null Fits? You Must Omit! How Measurement Artifacts Undermine Many Group Polarization Replications”."
  },
  {
    "objectID": "index.html#expectation-model-of-group-polarization-experiments",
    "href": "index.html#expectation-model-of-group-polarization-experiments",
    "title": "Notes and code examples to accompany “If the Null Fits, You Must Omit”",
    "section": "Expectation model of group polarization experiments",
    "text": "Expectation model of group polarization experiments\nSee Method section of the paper where we explain how to calculate the probability density vector representing the probability that a participant gives the opinion represented by each measurement bin, \\(b\\).\nHere we show how to calculate the mean observed opinion at pre- and post-discussion to show how spurious group polarization occurs under a null polarization hypothesis.\nMoscovici and Zavalloni (1969) report observing a mean opinion of -0.6 in pre-discussion opinions and mean opinion -1.04 post-discussion, measured using an ordinal scale from -3 (strongly disagree) to +3 (strongly agree). Let’s calculate the probability density vector for the pre- and post-discussion distributions then calculate the mean opinion for each one. The latent distributions have the same mean, -1.2, but different variances, which results in a shifted observed mean due to opinion clipping.\n\n\n\nMean group opinions pre- and post-discussion from Moscovici & Zavalloni (1969).\n\n\nWe call this a null polarization model (defined in terms of latent opinions) that generates spurious group polarization. Spurious group polarization is when the observed opinion distributions appear to polarize (i.e., radicalize, or become more extreme) that does notrepresent the true change in latent opinions. The measurement procedure clips extreme opinions less than the minimum bin value, or greater than the maximum, causing the average observed opinion to spuriously appear polarized.\nLet’s use some modeling code from the repository to demonstrate. For this part we need to source (use code from) the model.R file that has the the binProb function I use below. Also make sure you have set the working directory to the GroupPolarizationStatmod directory within the gp-stats project/repository directory.\nFor a preliminary step, adapt my code below for your computer. Change the argument to root.dir to be the location of the GroupPolarizationStatmod directory on your own computer.\nNow in the following chunk, the source function will know where to look for the file with the makeProbVec function we need, model.R.\n\n# This is how I set the working directory\n# Adapt for your own computer\n# setwd(\"~/workspace/gp-statmod/GroupPolarizationStatmod/\")\n# Load makeProbVec from file model.R\nsource(\"model.R\")\n\nNow read in the pre- and post-discussion means from Moscovici1969_Americans\n\n# Experimental means\nmosc1969_ave_pre &lt;- -0.6\nmosc1969_ave_post &lt;- -1.04\n\nCreate a hypothesized null model that will spuriously look like polarization, with the same pre- and post-discussion means above. These correspond to \\(\\mu\\), \\(\\sigma_\\mathrm{pre}\\), and \\(\\sigma_\\mathrm{post}\\) from the paper that define a null polarization model, \\(\\nu = (\\mu, \\sigma_\\mathrm{pre}, \\sigma_\\mathrm{post})\\).\n\n# Hypothesized null model that looks polarized\nmu &lt;- -1.2\nsd_pre &lt;- 4.3\nsd_post &lt;- 1.9\n\nThe function makeProbVec creates the vector \\(f[b]\\) of probabilities that bin \\(b_i \\in b\\) will be reported by a randomly-selected participant, given the measurement bin values and the distribution of latent opinions, assumed normal, defined by the mean and standard deviation, \\(\\mu\\) and \\(\\sigma\\). and bin values. Here bin values are \\(b=-3,-2,\\ldots,3\\); \\(\\mu=-1.2\\); and \\(\\sigma=4.3\\).\n\n# Vector w/ probability density for each bin\nf_pre &lt;- makeProbVec(-3:3, -1.2, 4.3)\n# Print to two significant digits\nsprintf(\"%.2f\", f_pre)\n\n[1] \"0.38\" \"0.09\" \"0.09\" \"0.09\" \"0.08\" \"0.07\" \"0.19\"\n\n\nLet’s barplot to have a look:\n\nbarplot(f_pre, \n        names.arg = -3:3,\n        ylim = c(0, 0.4),\n        ylab = \"Pct. Responding\",\n        xlab = \"Bin Value\")\n\n\n\n\n\n\n\n\nLet’s check the simulated average observed opinion, \\(\\bar{o}\\), using the expected value of the distribution, i.e., we assume \\(\\bar{o} = \\langle o \\rangle\\), so we use the standard formula for expected value, which assumes that \\(f[b]\\) is normalized.\n\\[\n\\bar{o} = \\sum_{b=b_1}^{b_B} b \\cdot f[b].\n\\]\nIn code,\n\n# Calculate simulated and compare with actual\nsim_obs_pre &lt;- sum((-3:3) * f_pre)\nprint(abs(sim_obs_pre - mosc1969_ave_pre))\n\n[1] 0.01195818\n\n\nThis is within one significant digit greater than the reporting scale, meaning it is an acceptable fit.\nThis shows that even just a little spread in the distribution of latent opinions results in a seriously distorted mean (Liddell and Kruschke 2018)—one that reproduces the mean reported by Moscovici and Zavalloni (1969), but actually confirming the null polarization hypothesis!\nLet’s check the simulated average observed opinion for post-deliberation, repeating the previous steps for \\(t=\\mathrm{post}\\).\n\nf_post &lt;- makeProbVec(-3:3, mu, sd_post)\nsprintf(\"%.2f\", f_post)\n\n[1] \"0.25\" \"0.19\" \"0.21\" \"0.17\" \"0.11\" \"0.05\" \"0.03\"\n\n# Calculate simulated and compare with actual\nsim_obs_post &lt;- sum((-3:3) * f_post)\nprint(abs(sim_obs_post - mosc1969_ave_post))\n\n[1] 0.00094857\n\n\nHere we get even closer agreement between the observed post-discussion average opinion in Moscovici and Zavalloni (1969) and the one we simulated."
  },
  {
    "objectID": "index.html#simulate-experimental-data",
    "href": "index.html#simulate-experimental-data",
    "title": "Notes and code examples to accompany “If the Null Fits, You Must Omit”",
    "section": "Simulate Experimental Data",
    "text": "Simulate Experimental Data\nI simulate experimental group polarization data for a given time, \\(t\\), with latent group opinion mean and standard deviation, \\(\\mu_t\\) and \\(\\sigma_t\\), and with a survey response scale defined by \\(B\\) bin values, \\(b_i\\), with \\(i=1,\\ldots,B\\)."
  },
  {
    "objectID": "index.html#simulate-each-experimental-designs-false-positive-rate",
    "href": "index.html#simulate-each-experimental-designs-false-positive-rate",
    "title": "Notes and code examples to accompany “If the Null Fits, You Must Omit”",
    "section": "Simulate each experimental design’s false positive rate",
    "text": "Simulate each experimental design’s false positive rate\nHere’s how to calculate the Type I or family-wise error rate for all the experiments (though I misnamed the function back in the day, so it calls all of the error rates “family-wise” because I had understood them to be synonyms).\n\nsource(\"src/fwer.R\")\n\nprobit_fits_dir &lt;- \"data/probit_fits\"\n# Set a \"moderate\" significance, according to Cohen's definitions \nsignificance_val &lt;- 0.5\nfwer_tbl &lt;- calculate_fwer(probit_fits_dir, significance_val);\n\nprint(head(fwer_tbl, n = 10))\n\n# A tibble: 10 × 3\n   StudyID       ExperimentID         FWER\n   &lt;chr&gt;         &lt;chr&gt;               &lt;dbl&gt;\n 1 Schkade2010   Boulder-CivilUnions 0.598\n 2 Burnstein1975 Experimental        0.406\n 3 Burnstein1973 B                   0.351\n 4 Abrams1990    Uncategorized3      0.349\n 5 Friedkin1999  Tetrads-Sports      0.342\n 6 Moscovici1969 Americans           0.335\n 7 Abrams1990    Uncategorized4      0.333\n 8 Burnstein1973 A                   0.319\n 9 Abrams1990    Uncategorized1      0.319\n10 Abrams1990    Categorized2        0.305\n\n\nThis is used inside the calc_fdr_vs_significance function that adds another column to this dataframe, the false discovery rate (FDR) for a set of significance values. Here it is being called with the default metascience parameters with a base rate of \\(b=0.1\\) and a statistical power of \\(W=0.8\\). We consider three effect-detection thresholds in the paper: “small”, “medium”, and “large” effect sizes traditionally assumed practically meaningful across social sciences (Cohen 1988).\nThe calc_fdr_vs_significance function calculates the false-positive rate (\\(\\alpha\\)) and false detection rate (\\(\\mathrm{FDR}\\)) reported for a variety of effect-detection thresholds, \\(d^*\\), as we call them in the paper. When I first wrote the code, though, I was thinking about detection thresholds as synonymous with “significance values”. So that’s why the function name, calc_fdr_vs_significance, includes the word significance, which is a stand-in for the maximally descriptive “effect-detection thresholds” terminology. This seemed fine to keep things simple and not change the name everywhere in the code. But it was right to change it for the paper, since significance is only understood in a p-value based null-hypothesis significance testing approach, where a hypothesis is confirmed if the p-value is less than some particular threshold. I updated the prose in the article to reflect this distinction, but left the code as-is.\nI put this function in the scripts directory because to run the full analyses presented in the paper it’s only necessary to run calc_fdr_vs_significance since it creates a column for the false positive rate and the estimated false detection rate calculated for each experimental design we studied.\nBelow I call the function with the defaults specified, for illustration. Remember the significance_vals map to effect-detection thresholds (\\(d^*\\)) in the paper.\n\nsource(\"scripts/analysis.R\")\n\n# This \nfdr_over_thresholds &lt;- calc_fdr_vs_significance(\n  probit_fits_dir = probit_fits_dir,  # defined in above code block\n  base_rate = 0.1, power = 0.8, significance_vals = c(0.2, 0.5, 0.8)\n)\n\nprint(head(fdr_over_thresholds, n=10))\n\n# A tibble: 10 × 7\n   StudyID       ExperimentID         FWER   FDR SigVal Power BaseRate\n   &lt;chr&gt;         &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 Schkade2010   Boulder-CivilUnions 0.69  0.886    0.2   0.8      0.1\n 2 Burnstein1975 Experimental        0.518 0.854    0.2   0.8      0.1\n 3 Abrams1990    Uncategorized3      0.471 0.841    0.2   0.8      0.1\n 4 Abrams1990    Uncategorized4      0.459 0.838    0.2   0.8      0.1\n 5 Burnstein1973 B                   0.438 0.831    0.2   0.8      0.1\n 6 Abrams1990    Uncategorized1      0.434 0.830    0.2   0.8      0.1\n 7 Krizan2007    NoOutgroupScenario2 0.423 0.826    0.2   0.8      0.1\n 8 Friedkin1999  Tetrads-Sports      0.417 0.824    0.2   0.8      0.1\n 9 Abrams1990    Uncategorized5      0.414 0.823    0.2   0.8      0.1\n10 Krizan2007    NoOutgroupScenario1 0.41  0.822    0.2   0.8      0.1\n\n\nTo see SigVal = 0.8 (ie \\(d^* = 0.8\\)), check the tail of the table:\n\nprint(tail(fdr_over_thresholds, n = 10))\n\n# A tibble: 10 × 7\n   StudyID       ExperimentID             FWER   FDR SigVal Power BaseRate\n   &lt;chr&gt;         &lt;chr&gt;                   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 Hogg1990      Risky-Neutral          0.072  0.448    0.8   0.8      0.1\n 2 Hogg1990      Neutral-Cautious       0.0680 0.433    0.8   0.8      0.1\n 3 Myers1975     Feminists-Experimental 0.0630 0.415    0.8   0.8      0.1\n 4 Friedkin1999  Dyads-Surgery          0.0540 0.378    0.8   0.8      0.1\n 5 Schkade2010   COSprings-CivilUnions  0.038  0.299    0.8   0.8      0.1\n 6 Friedkin1999  Triads-School          0.0360 0.288    0.8   0.8      0.1\n 7 Friedkin1999  Triads-Sports          0.0340 0.277    0.8   0.8      0.1\n 8 Burnstein1973 F                      0.0330 0.271    0.8   0.8      0.1\n 9 Friedkin1999  Tetrads-Surgery        0.0290 0.246    0.8   0.8      0.1\n10 Friedkin1999  Tetrads-School         0.027  0.233    0.8   0.8      0.1"
  },
  {
    "objectID": "index.html#find-effect-detection-thresholds-to-achieve-alpha-0.05",
    "href": "index.html#find-effect-detection-thresholds-to-achieve-alpha-0.05",
    "title": "Notes and code examples to accompany “If the Null Fits, You Must Omit”",
    "section": "Find effect-detection thresholds to achieve \\(\\alpha = 0.05\\)",
    "text": "Find effect-detection thresholds to achieve \\(\\alpha = 0.05\\)\nIn the second step of our generative model analysis I calculate the solution to the problem in Equation 2, finding the effect-detection threshold, \\(d^*\\), that achieves a target false positive rate, \\(\\alpha^* = 0.05\\)."
  },
  {
    "objectID": "index.html#identify-invalid-replications-all-but-5-were-invalidated",
    "href": "index.html#identify-invalid-replications-all-but-5-were-invalidated",
    "title": "Notes and code examples to accompany “If the Null Fits, You Must Omit”",
    "section": "Identify Invalid Replications: All But 5% Were Invalidated",
    "text": "Identify Invalid Replications: All But 5% Were Invalidated"
  },
  {
    "objectID": "index.html#evaluate-experimental-designs-70-chance-a-polarization-detection-was-erroneous",
    "href": "index.html#evaluate-experimental-designs-70-chance-a-polarization-detection-was-erroneous",
    "title": "Notes and code examples to accompany “If the Null Fits, You Must Omit”",
    "section": "Evaluate Experimental Designs: ~70% Chance a Polarization Detection was Erroneous",
    "text": "Evaluate Experimental Designs: ~70% Chance a Polarization Detection was Erroneous"
  },
  {
    "objectID": "index.html#specify-effect-detection-thresholds-spec-experimentsdont-just-design-themand-avoid-informed-guesswork",
    "href": "index.html#specify-effect-detection-thresholds-spec-experimentsdont-just-design-themand-avoid-informed-guesswork",
    "title": "Notes and code examples to accompany “If the Null Fits, You Must Omit”",
    "section": "Specify Effect-Detection Thresholds: Spec Experiments—Don’t Just Design Them—and Avoid Informed Guesswork",
    "text": "Specify Effect-Detection Thresholds: Spec Experiments—Don’t Just Design Them—and Avoid Informed Guesswork"
  }
]